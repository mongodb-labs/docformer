{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train GPT Model on JSON data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and Setup\n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel, AutoConfig, TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "import psutil\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Check if MPS is available\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from docformer.data import (\n",
    "    DocPermuterPipe,\n",
    "    DocTokenizerPipe,\n",
    "    PadTruncTokensPipe,\n",
    "    SchemaParserPipe,\n",
    "    TargetFieldPipe,\n",
    "    TokenEncoderPipe,\n",
    "    UpscalerPipe,\n",
    "    load_df_from_mongodb,\n",
    ")\n",
    "\n",
    "TARGET_FIELD = \"problem\"\n",
    "\n",
    "# load data into dataframe and split into train/test\n",
    "df = load_df_from_mongodb(\n",
    "    \"mongodb://localhost:27017\",\n",
    "    \"codenet\",\n",
    "    \"train\",\n",
    "    projection={\"_id\": 0, \"filePath\": 0},\n",
    ")\n",
    "train_docs_df, test_docs_df = train_test_split(df, test_size=0.2, shuffle=True)\n",
    "\n",
    "# create train and test pipelines\n",
    "pipes = {\n",
    "    # --- train only ---\n",
    "    \"schema\": SchemaParserPipe(),\n",
    "    # \"upscaler\": UpscalerPipe(n=2),\n",
    "    \"permuter\": DocPermuterPipe(shuffle_arrays=True),\n",
    "    # --- test only ---\n",
    "    \"target\": TargetFieldPipe(TARGET_FIELD),\n",
    "    # --- train and test ---\n",
    "    \"tokenizer\": DocTokenizerPipe(path_in_field_tokens=False),\n",
    "    \"padding\": PadTruncTokensPipe(length=4000),\n",
    "    \"encoder\": TokenEncoderPipe(max_tokens=4000),\n",
    "}\n",
    "\n",
    "train_pipeline = Pipeline(\n",
    "    [(name, pipes[name]) for name in (\"target\", \"permuter\", \"tokenizer\", \"padding\", \"encoder\")], verbose=True\n",
    ")\n",
    "test_pipeline = Pipeline([(name, pipes[name]) for name in (\"target\", \"tokenizer\", \"padding\", \"encoder\")], verbose=True)\n",
    "\n",
    "# process train, eval and test data\n",
    "train_df = train_pipeline.fit_transform(train_docs_df)\n",
    "test_df = test_pipeline.transform(test_docs_df)\n",
    "\n",
    "# allow transitions in vpda to work for test data\n",
    "pipes[\"schema\"].fit(test_docs_df)\n",
    "\n",
    "# drop ordered_docs columns to save space\n",
    "train_df.drop(columns=[\"docs\"], inplace=True)\n",
    "test_df.drop(columns=[\"docs\"], inplace=True)\n",
    "\n",
    "# drop all rows where the tokens array doesn't end in 0 (longer than max)\n",
    "train_df = train_df[train_df[\"tokens\"].apply(lambda x: x[-1] == 0)]\n",
    "test_df = test_df[test_df[\"tokens\"].apply(lambda x: x[-1] == 0)]\n",
    "\n",
    "# get stateful objects\n",
    "# schema = pipes[\"schema\"].schema\n",
    "encoder = pipes[\"encoder\"].encoder\n",
    "block_size = pipes[\"padding\"].length\n",
    "\n",
    "# print data stats\n",
    "print(f\"len train: {len(train_df)}, len test: {len(test_df)}\")\n",
    "print(f\"vocab size {encoder.vocab_size}\")\n",
    "print(f\"block size {block_size}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from docformer.utils import Symbol\n",
    "\n",
    "# Create custom config\n",
    "config = AutoConfig.from_pretrained(\n",
    "    \"gpt2\",\n",
    "    vocab_size=encoder.vocab_size,\n",
    "    n_ctx=block_size,\n",
    "    bos_token_id=encoder.encode(Symbol.START),\n",
    "    eos_token_id=encoder.encode(Symbol.END),\n",
    "    pad_token_id=encoder.encode(Symbol.PAD),\n",
    "    hidden_size=128, \n",
    "    num_attention_heads=4,\n",
    "    num_hidden_layers=4,\n",
    ")\n",
    "\n",
    "model = GPT2LMHeadModel(config).to(device)\n",
    "model.resize_token_embeddings(encoder.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare HuggingFace Dataset\n",
    "train_df[\"input_ids\"] = train_df[\"tokens\"]\n",
    "test_df[\"input_ids\"] = test_df[\"tokens\"]\n",
    "\n",
    "train_df.drop(columns=[\"tokens\", \"docs\", \"id\"], inplace=True)\n",
    "test_df.drop(columns=[\"tokens\", \"docs\", \"id\"], inplace=True)\n",
    "\n",
    "# create HuggingFace datasets from Pandas DataFrames\n",
    "train_tokenized = Dataset.from_pandas(train_df)\n",
    "test_tokenized = Dataset.from_pandas(test_df)\n",
    "\n",
    "# # set format to Torch\n",
    "train_tokenized.set_format(\"torch\", columns=[\"input_ids\"])\n",
    "test_tokenized.set_format(\"torch\", columns=[\"input_ids\"])\n",
    "\n",
    "\n",
    "train_tokenized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "dummy_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "dummy_tokenizer._pad_token = \"<PAD>\"\n",
    "dummy_tokenizer.pad_token_id = encoder.encode(Symbol.PAD)\n",
    "dummy_tokenizer.padding_side = \"right\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainerCallback\n",
    "\n",
    "class LossCallback(TrainerCallback):\n",
    "    def __init__(self):\n",
    "        self.training_loss = []\n",
    "        self.step = []\n",
    "\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if state.is_local_process_zero and 'loss' in logs:\n",
    "            self.training_loss.append(logs['loss'])\n",
    "            self.step.append(state.global_step)\n",
    "            self.plot_loss()\n",
    "\n",
    "    def plot_loss(self):\n",
    "        clear_output(wait=True)\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(self.step, self.training_loss)\n",
    "        plt.title('Training Loss Curve')\n",
    "        plt.xlabel('Step')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.show()\n",
    "\n",
    "    # Implement other required methods as no-ops\n",
    "    def on_init_end(self, args, state, control, **kwargs):\n",
    "        pass\n",
    "\n",
    "    def on_train_begin(self, args, state, control, **kwargs):\n",
    "        pass\n",
    "\n",
    "    def on_train_end(self, args, state, control, **kwargs):\n",
    "        pass\n",
    "\n",
    "loss_callback = LossCallback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set up training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./checkpoints\",\n",
    "    num_train_epochs=4,\n",
    "    per_device_train_batch_size=100,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    learning_rate=5e-4,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=100,\n",
    "    report_to=\"none\",  # Disable wandb logging\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=dummy_tokenizer, mlm=False)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_tokenized,\n",
    "    eval_dataset=test_tokenized,\n",
    "    data_collator=data_collator,\n",
    "    callbacks=[loss_callback]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train model\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "print(\"Training completed!\")\n",
    "loss_callback.plot_loss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save the model\n",
    "model.save_pretrained(\"./checkpoints/checkpoint-final\")\n",
    "print(\"Model saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model from pretrained\n",
    "model = GPT2LMHeadModel.from_pretrained(\"./checkpoints/checkpoint-final\").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Set up evaluation arguments\n",
    "eval_args = TrainingArguments(\n",
    "    output_dir=\"./checkpoints\",\n",
    "    do_train=False,\n",
    "    do_eval=True,\n",
    "    per_device_eval_batch_size=32,\n",
    "    dataloader_drop_last=False\n",
    ")\n",
    "\n",
    "# Create a Trainer instance for evaluation\n",
    "eval_trainer = Trainer(\n",
    "    model=model,\n",
    "    args=eval_args,\n",
    "    eval_dataset=test_tokenized,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "# Run evaluation\n",
    "eval_results = eval_trainer.evaluate()\n",
    "\n",
    "# Print evaluation results\n",
    "print(\"Evaluation Results:\")\n",
    "for key, value in eval_results.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "# Calculate perplexity\n",
    "perplexity = np.exp(eval_results['eval_loss'])\n",
    "print(f\"Perplexity: {perplexity}\")\n",
    "\n",
    "# Generate some text as a sanity check\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "_ = model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docformer.utils import FieldToken\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "\n",
    "def truncate(tensor, truncate_id):\n",
    "    index = (tensor == truncate_id).nonzero()\n",
    "    if index.numel() > 0:\n",
    "        return tensor[:index[-1] + 1]\n",
    "    return tensor\n",
    "\n",
    "def predict_label(input_ids: torch.Tensor, model):\n",
    "    target_token_id = encoder.encode(FieldToken(TARGET_FIELD))\n",
    "    trunc_ids = truncate(input_ids, target_token_id).view(1, -1).to(model.device)\n",
    "\n",
    "    # Assuming trunc_ids is your truncated tensor of input IDs\n",
    "    attention_mask = torch.ones_like(trunc_ids).to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            trunc_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_new_tokens=1\n",
    "        )\n",
    "\n",
    "    pred = encoder.decode(output[0,-1])\n",
    "    return pred\n",
    "\n",
    "\n",
    "predictions = [predict_label(doc, model) for doc in tqdm(test_tokenized[\"input_ids\"])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    " \n",
    "# Load metric\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "predictions_int = [encoder.encode(pred) for pred in predictions]\n",
    "references_int = [encoder.encode(ref) for ref in test_tokenized[\"target\"]]\n",
    "\n",
    "# Compute metrics\n",
    "results = accuracy_metric.compute(predictions=predictions_int, references=references_int)\n",
    "\n",
    "print(\"results: \", results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
