{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train GPT Model on JSON data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# include ../ in path\n",
    "import sys\n",
    "sys.path.insert(0, '../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and Setup\n",
    "import torch\n",
    "from transformers import AutoTokenizer, GPT2LMHeadModel, AutoConfig, TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "from datasets import Dataset\n",
    "from tqdm.notebook import tqdm\n",
    "import psutil\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Check if MPS is available\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docformer.data import load_from_mongodb\n",
    "\n",
    "train_dataset, test_dataset = load_from_mongodb(\"mongodb://localhost:27017/\", \n",
    "                                                db_name=\"openml\", coll_name=\"1590-adult\", \n",
    "                                                target_field=\"target\", test_size=0.2, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docformer.data import tokenize\n",
    "\n",
    "# Load and prepare tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = 'right'\n",
    "\n",
    "# tokenize datasets\n",
    "train_tokenized = tokenize(tokenizer, train_dataset)\n",
    "test_tokenized = tokenize(tokenizer, test_dataset)\n",
    "\n",
    "\n",
    "# Create custom config\n",
    "config = AutoConfig.from_pretrained(\n",
    "    \"gpt2\",\n",
    "    vocab_size=len(tokenizer),\n",
    "    n_ctx=128,\n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    hidden_size=128, \n",
    "    num_attention_heads=4,\n",
    "    num_hidden_layers=4,\n",
    ")\n",
    "\n",
    "model = GPT2LMHeadModel(config).to(device)\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainerCallback\n",
    "\n",
    "class LossCallback(TrainerCallback):\n",
    "    def __init__(self):\n",
    "        self.training_loss = []\n",
    "        self.step = []\n",
    "\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if state.is_local_process_zero and 'loss' in logs:\n",
    "            self.training_loss.append(logs['loss'])\n",
    "            self.step.append(state.global_step)\n",
    "            self.plot_loss()\n",
    "\n",
    "    def plot_loss(self):\n",
    "        clear_output(wait=True)\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(self.step, self.training_loss)\n",
    "        plt.title('Training Loss Curve')\n",
    "        plt.xlabel('Step')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.show()\n",
    "\n",
    "    # Implement other required methods as no-ops\n",
    "    def on_init_end(self, args, state, control, **kwargs):\n",
    "        pass\n",
    "\n",
    "    def on_train_begin(self, args, state, control, **kwargs):\n",
    "        pass\n",
    "\n",
    "    def on_train_end(self, args, state, control, **kwargs):\n",
    "        pass\n",
    "\n",
    "loss_callback = LossCallback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set up training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./checkpoints\",\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=16,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    report_to=\"none\",  # Disable wandb logging\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_tokenized,\n",
    "    eval_dataset=test_tokenized,\n",
    "    data_collator=data_collator,\n",
    "    callbacks=[loss_callback]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train model\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "print(\"Training completed!\")\n",
    "loss_callback.plot_loss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save the model\n",
    "model.save_pretrained(\"./checkpoints/checkpoint-final\")\n",
    "tokenizer.save_pretrained(\"./checkpoints/checkpoint-final\")\n",
    "print(\"Model saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Memory cleanup\n",
    "del model, trainer, tokenizer, train_tokenized, test_tokenized, dataset, train_dataset, test_dataset, dataset_split\n",
    "gc.collect()\n",
    "print(f\"Current memory usage: {psutil.virtual_memory().percent}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained(\"./checkpoints/checkpoint-final\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./checkpoints/checkpoint-final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Set up evaluation arguments\n",
    "eval_args = TrainingArguments(\n",
    "    output_dir=\"./eval_results\",\n",
    "    do_train=False,\n",
    "    do_eval=True,\n",
    "    per_device_eval_batch_size=32,\n",
    "    dataloader_drop_last=False\n",
    ")\n",
    "\n",
    "# Create a Trainer instance for evaluation\n",
    "eval_trainer = Trainer(\n",
    "    model=model,\n",
    "    args=eval_args,\n",
    "    eval_dataset=test_tokenized,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "# Run evaluation\n",
    "eval_results = eval_trainer.evaluate()\n",
    "\n",
    "# Print evaluation results\n",
    "print(\"Evaluation Results:\")\n",
    "for key, value in eval_results.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "# Calculate perplexity\n",
    "perplexity = np.exp(eval_results['eval_loss'])\n",
    "print(f\"Perplexity: {perplexity}\")\n",
    "\n",
    "# Generate some text as a sanity check\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "_ = model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docformer.eval_batch import evaluate_gpt2_classification\n",
    "\n",
    "model_path = \"./checkpoints/checkpoint-final\"\n",
    "results = evaluate_gpt2_classification(model_path, test_dataset)\n",
    "print(f\"Classification Accuracy: {results['accuracy']:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
