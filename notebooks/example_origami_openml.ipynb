{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of instances: 1000\n",
      "Target field for classification: \"class\"\n",
      "Example instance:\n",
      "{'checking_status': '<0', 'duration': 6, 'credit_history': 'critical/other existing credit', 'purpose': 'radio/tv', 'credit_amount': 1169.0, 'savings_status': 'no known savings', 'employment': '>=7', 'installment_commitment': 4, 'personal_status': 'male single', 'other_parties': 'none', 'residence_since': 4, 'property_magnitude': 'real estate', 'age': 67, 'other_payment_plans': 'none', 'housing': 'own', 'existing_credits': 2, 'job': 'skilled', 'num_dependents': 1, 'own_telephone': 'yes', 'foreign_worker': 'yes', 'class': 'good'}\n"
     ]
    }
   ],
   "source": [
    "from openml import datasets\n",
    "\n",
    "# Fetch dataset by ID\n",
    "dataset = datasets.get_dataset(31)\n",
    "\n",
    "# Get the data in pandas DataFrame format\n",
    "X, _, _, _ = dataset.get_data()\n",
    "\n",
    "# drop fnlwgt column\n",
    "# X = X.drop(\"fnlwgt\", axis=1)\n",
    "\n",
    "# Convert DataFrame to list of dictionaries\n",
    "data = X.to_dict(\"records\")\n",
    "\n",
    "# get the name of the target label\n",
    "target_field = dataset.default_target_attribute\n",
    "\n",
    "print(f\"Number of instances: {len(data)}\")\n",
    "print(f'Target field for classification: \"{target_field}\"')\n",
    "print(f\"Example instance:\\n{data[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train pipeline: Pipeline(steps=[('binning',\n",
      "                 KBinsDiscretizerPipe(strategy='kmeans', threshold=100)),\n",
      "                ('target', TargetFieldPipe(target_field='class')),\n",
      "                ('schema', SchemaParserPipe()),\n",
      "                ('tokenizer', DocTokenizerPipe()),\n",
      "                ('padding', PadTruncTokensPipe()),\n",
      "                ('encoder', TokenEncoderPipe(max_tokens=0))],\n",
      "         verbose=True)\n",
      "test pipeline: Pipeline(steps=[('binning',\n",
      "                 KBinsDiscretizerPipe(strategy='kmeans', threshold=100)),\n",
      "                ('target', TargetFieldPipe(target_field='class')),\n",
      "                ('tokenizer', DocTokenizerPipe()),\n",
      "                ('padding', PadTruncTokensPipe()),\n",
      "                ('encoder', TokenEncoderPipe(max_tokens=0))],\n",
      "         verbose=True)\n",
      "[Pipeline] ........... (step 1 of 6) Processing binning, total=   0.2s\n",
      "[Pipeline] ............ (step 2 of 6) Processing target, total=   0.0s\n",
      "[Pipeline] ............ (step 3 of 6) Processing schema, total=   0.0s\n",
      "[Pipeline] ......... (step 4 of 6) Processing tokenizer, total=   0.0s\n",
      "[Pipeline] ........... (step 5 of 6) Processing padding, total=   0.0s\n",
      "[Pipeline] ........... (step 6 of 6) Processing encoder, total=   0.1s\n",
      "len train: 800, len test: 200\n",
      "vocab size 254\n",
      "block size 45\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from origami.preprocessing import build_prediction_pipelines, docs_to_df\n",
    "from origami.utils import set_seed\n",
    "from origami.utils.config import TopLevelConfig\n",
    "\n",
    "# for reproducibility\n",
    "set_seed(123)\n",
    "\n",
    "# load data into \"docs\" column in dataframe and split into train/test\n",
    "df = docs_to_df(data)\n",
    "train_docs_df, test_docs_df = train_test_split(df, test_size=0.2, shuffle=True)\n",
    "\n",
    "config = TopLevelConfig()\n",
    "\n",
    "# pipeline config\n",
    "config.pipeline.upscale = 100\n",
    "config.pipeline.sequence_order = \"SHUFFLED\"\n",
    "\n",
    "# create train and test pipelines\n",
    "pipelines = build_prediction_pipelines(pipeline_config=config.pipeline, target_field=target_field, verbose=True)\n",
    "\n",
    "# process train, eval and test data\n",
    "train_df = pipelines[\"train\"].fit_transform(train_docs_df)\n",
    "test_df = pipelines[\"test\"].transform(test_docs_df)\n",
    "\n",
    "# get stateful objects\n",
    "schema = pipelines[\"train\"][\"schema\"].schema\n",
    "encoder = pipelines[\"train\"][\"encoder\"].encoder\n",
    "block_size = pipelines[\"train\"][\"padding\"].length\n",
    "\n",
    "# print data stats\n",
    "print(f\"len train: {len(train_df)}, len test: {len(test_df)}\")\n",
    "print(f\"vocab size {encoder.vocab_size}\")\n",
    "print(f\"block size {block_size}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 0.86M\n"
     ]
    }
   ],
   "source": [
    "from origami.model import ORIGAMI\n",
    "from origami.model.vpda import ObjectVPDA\n",
    "from origami.preprocessing import DFDataset\n",
    "from origami.utils import count_parameters\n",
    "\n",
    "# wrap dataframes in datasets\n",
    "train_dataset = DFDataset(train_df)\n",
    "test_dataset = DFDataset(test_df)\n",
    "\n",
    "# model config\n",
    "config.model.n_layer = 4\n",
    "config.model.n_head = 4\n",
    "config.model.n_embd = 64\n",
    "config.model.vocab_size = encoder.vocab_size\n",
    "config.model.block_size = block_size\n",
    "\n",
    "# create PDA and pass it to the model\n",
    "vpda = ObjectVPDA(encoder, schema)\n",
    "model = ORIGAMI(config.model, config.train, vpda=vpda)\n",
    "\n",
    "n_params = count_parameters(model)\n",
    "print(f\"Number of parameters: {n_params / 1e6:.2f}M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|  step: 0  |  epoch: 0  |  batch_num: 0  |  batch_dt: 0.00  |  batch_loss: 2.7793  |  lr: 1.01e-06  |  train_acc: 0.0000  |  test_loss: 2.7882  |  test_acc: 0.0000  |\n",
      "|  step: 1  |  epoch: 12  |  batch_num: 100  |  batch_dt: 75.38  |  batch_loss: 0.9688  |  lr: 1.01e-04  |\n",
      "|  step: 2  |  epoch: 25  |  batch_num: 200  |  batch_dt: 72.75  |  batch_loss: 0.6550  |  lr: 2.01e-04  |\n",
      "|  step: 3  |  epoch: 37  |  batch_num: 300  |  batch_dt: 72.20  |  batch_loss: 0.6319  |  lr: 3.01e-04  |\n",
      "|  step: 4  |  epoch: 50  |  batch_num: 400  |  batch_dt: 73.25  |  batch_loss: 0.5899  |  lr: 4.01e-04  |\n",
      "|  step: 5  |  epoch: 62  |  batch_num: 500  |  batch_dt: 73.57  |  batch_loss: 0.5710  |  lr: 5.01e-04  |\n",
      "|  step: 6  |  epoch: 75  |  batch_num: 600  |  batch_dt: 73.87  |  batch_loss: 0.5103  |  lr: 6.01e-04  |\n",
      "|  step: 7  |  epoch: 87  |  batch_num: 700  |  batch_dt: 74.45  |  batch_loss: 0.4682  |  lr: 7.01e-04  |\n",
      "|  step: 8  |  epoch: 100  |  batch_num: 800  |  batch_dt: 73.61  |  batch_loss: 0.3980  |  lr: 8.01e-04  |\n",
      "|  step: 9  |  epoch: 112  |  batch_num: 900  |  batch_dt: 73.51  |  batch_loss: 0.3289  |  lr: 9.01e-04  |\n",
      "|  step: 10  |  epoch: 125  |  batch_num: 1000  |  batch_dt: 73.51  |  batch_loss: 0.2830  |  lr: 1.00e-03  |  train_acc: 0.9900  |  test_loss: 0.9486  |  test_acc: 0.6700  |\n",
      "|  step: 11  |  epoch: 137  |  batch_num: 1100  |  batch_dt: 72.86  |  batch_loss: 0.2531  |  lr: 9.91e-04  |\n",
      "|  step: 12  |  epoch: 150  |  batch_num: 1200  |  batch_dt: 79.07  |  batch_loss: 0.2347  |  lr: 9.82e-04  |\n",
      "|  step: 13  |  epoch: 162  |  batch_num: 1300  |  batch_dt: 74.02  |  batch_loss: 0.2107  |  lr: 9.73e-04  |\n",
      "|  step: 14  |  epoch: 175  |  batch_num: 1400  |  batch_dt: 72.40  |  batch_loss: 0.2048  |  lr: 9.64e-04  |\n",
      "|  step: 15  |  epoch: 187  |  batch_num: 1500  |  batch_dt: 76.68  |  batch_loss: 0.2026  |  lr: 9.55e-04  |\n",
      "|  step: 16  |  epoch: 200  |  batch_num: 1600  |  batch_dt: 73.64  |  batch_loss: 0.1931  |  lr: 9.46e-04  |\n",
      "|  step: 17  |  epoch: 212  |  batch_num: 1700  |  batch_dt: 79.86  |  batch_loss: 0.1905  |  lr: 9.37e-04  |\n",
      "|  step: 18  |  epoch: 225  |  batch_num: 1800  |  batch_dt: 73.60  |  batch_loss: 0.1933  |  lr: 9.28e-04  |\n",
      "|  step: 19  |  epoch: 237  |  batch_num: 1900  |  batch_dt: 76.62  |  batch_loss: 0.1831  |  lr: 9.19e-04  |\n",
      "|  step: 20  |  epoch: 250  |  batch_num: 2000  |  batch_dt: 74.13  |  batch_loss: 0.1863  |  lr: 9.10e-04  |  train_acc: 1.0000  |  test_loss: 1.4850  |  test_acc: 0.7300  |\n",
      "model training interrupted after 258 epochs (2066 batches total).\n"
     ]
    }
   ],
   "source": [
    "from origami.inference import Predictor\n",
    "from origami.utils import make_progress_callback\n",
    "\n",
    "# create a predictor\n",
    "predictor = Predictor(model, encoder, target_field)\n",
    "\n",
    "# create and register progress callback\n",
    "progress_callback = make_progress_callback(\n",
    "    config.train, train_dataset=train_dataset, test_dataset=test_dataset, predictor=predictor\n",
    ")\n",
    "model.set_callback(\"on_batch_end\", progress_callback)\n",
    "\n",
    "# train model\n",
    "model.train_model(train_dataset, batches=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae99a6ec52394732a30fb12bbc3eb82c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.9948\n",
      "Model predictions (first 10):  ['positive', 'positive', 'positive', 'positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'negative']\n",
      "Correct labels (first 10):  ['positive', 'positive', 'positive', 'positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'negative']\n"
     ]
    }
   ],
   "source": [
    "# calculate test accuracy\n",
    "acc = predictor.accuracy(test_dataset, show_progress=True)\n",
    "print(f\"Test accuracy: {acc:.4f}\")\n",
    "\n",
    "# we can also access the predictions with the `predict()` method\n",
    "predictions = predictor.predict(test_dataset)\n",
    "print(\"Model predictions (first 10): \", predictions[:10])\n",
    "print(\"Correct labels (first 10): \", test_dataset.df[\"target\"].to_list()[:10])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
