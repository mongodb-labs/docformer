{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train ORIGAMI model on Dungeons dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"door\": 4,\n",
      "  \"key_color\": \"red\",\n",
      "  \"corridor\": [\n",
      "    {\n",
      "      \"monsters\": [\n",
      "        \"orc\",\n",
      "        \"troll\"\n",
      "      ],\n",
      "      \"door_no\": 0,\n",
      "      \"red_key\": \"spellbooks\",\n",
      "      \"blue_key\": \"spellbooks\",\n",
      "      \"green_key\": \"gold\"\n",
      "    },\n",
      "    {\n",
      "      \"door_no\": 1,\n",
      "      \"red_key\": \"artifacts\",\n",
      "      \"blue_key\": \"gold\",\n",
      "      \"green_key\": \"artifacts\"\n",
      "    },\n",
      "    {\n",
      "      \"monsters\": [\n",
      "        \"troll\"\n",
      "      ],\n",
      "      \"door_no\": 2,\n",
      "      \"red_key\": \"diamonds\",\n",
      "      \"blue_key\": \"spellbooks\",\n",
      "      \"green_key\": \"spellbooks\"\n",
      "    },\n",
      "    {\n",
      "      \"door_no\": 3,\n",
      "      \"red_key\": \"artifacts\",\n",
      "      \"blue_key\": \"spellbooks\",\n",
      "      \"green_key\": \"gemstones\"\n",
      "    },\n",
      "    {\n",
      "      \"monsters\": [\n",
      "        \"wolf\"\n",
      "      ],\n",
      "      \"door_no\": 4,\n",
      "      \"red_key\": \"spellbooks\",\n",
      "      \"blue_key\": \"gold\",\n",
      "      \"green_key\": \"gemstones\"\n",
      "    },\n",
      "    {\n",
      "      \"door_no\": 5,\n",
      "      \"red_key\": \"diamonds\",\n",
      "      \"blue_key\": \"diamonds\",\n",
      "      \"green_key\": \"diamonds\"\n",
      "    },\n",
      "    {\n",
      "      \"monsters\": [\n",
      "        \"wolf\",\n",
      "        \"troll\"\n",
      "      ],\n",
      "      \"door_no\": 6,\n",
      "      \"red_key\": \"diamonds\",\n",
      "      \"blue_key\": \"gold\",\n",
      "      \"green_key\": \"gold\"\n",
      "    },\n",
      "    {\n",
      "      \"monsters\": [\n",
      "        \"troll\"\n",
      "      ],\n",
      "      \"door_no\": 7,\n",
      "      \"red_key\": \"diamonds\",\n",
      "      \"blue_key\": \"gold\",\n",
      "      \"green_key\": \"diamonds\"\n",
      "    },\n",
      "    {\n",
      "      \"monsters\": [\n",
      "        \"troll\"\n",
      "      ],\n",
      "      \"door_no\": 8,\n",
      "      \"red_key\": \"diamonds\",\n",
      "      \"blue_key\": \"artifacts\",\n",
      "      \"green_key\": \"gemstones\"\n",
      "    },\n",
      "    {\n",
      "      \"monsters\": [\n",
      "        \"troll\",\n",
      "        \"orc\"\n",
      "      ],\n",
      "      \"door_no\": 9,\n",
      "      \"red_key\": \"diamonds\",\n",
      "      \"blue_key\": \"spellbooks\",\n",
      "      \"green_key\": \"gemstones\"\n",
      "    }\n",
      "  ],\n",
      "  \"treasure\": \"spellbooks\"\n",
      "}\n",
      "len train: 8000, len test: 2000\n",
      "vocab size 53\n",
      "block size 150\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from origami.datasets.dungeons import generate_data\n",
    "from origami.preprocessing import (\n",
    "    DocTokenizerPipe,\n",
    "    PadTruncTokensPipe,\n",
    "    SchemaParserPipe,\n",
    "    TargetFieldPipe,\n",
    "    TokenEncoderPipe,\n",
    "    docs_to_df,\n",
    ")\n",
    "\n",
    "# generate Dungeons dataset (see origami/datasets/dungeons.py)\n",
    "data = generate_data(\n",
    "    num_instances=10_000,\n",
    "    num_doors_range=(5, 10),\n",
    "    num_colors=3,\n",
    "    with_monsters=True,\n",
    "    num_treasures=5,\n",
    "    shuffle_doors=True\n",
    ")\n",
    "\n",
    "# print example dictionary\n",
    "print(json.dumps(data[0], indent=2))\n",
    "\n",
    "# load data into dataframe and split into train/test\n",
    "df = docs_to_df(data)\n",
    "train_docs_df, test_docs_df = train_test_split(df, test_size=0.2, shuffle=True)\n",
    "\n",
    "TARGET_FIELD = \"treasure\"\n",
    "\n",
    "# create train and test pipelines\n",
    "pipes = {\n",
    "    \"schema\": SchemaParserPipe(),\n",
    "    \"target\": TargetFieldPipe(TARGET_FIELD),\n",
    "    \"tokenizer\": DocTokenizerPipe(path_in_field_tokens=True),\n",
    "    \"padding\": PadTruncTokensPipe(length=\"max\"),\n",
    "    \"encoder\": TokenEncoderPipe(),\n",
    "}\n",
    "\n",
    "pipeline = Pipeline([(name, pipes[name]) for name in (\"schema\", \"target\", \"tokenizer\", \"padding\", \"encoder\")])\n",
    "\n",
    "# process train, eval and test data\n",
    "train_df = pipeline.fit_transform(train_docs_df)\n",
    "test_df = pipeline.transform(test_docs_df)\n",
    "\n",
    "# get stateful objects\n",
    "schema = pipes[\"schema\"].schema\n",
    "encoder = pipes[\"encoder\"].encoder\n",
    "block_size = pipes[\"padding\"].length\n",
    "\n",
    "# print data stats\n",
    "print(f\"len train: {len(train_df)}, len test: {len(test_df)}\")\n",
    "print(f\"vocab size {encoder.vocab_size}\")\n",
    "print(f\"block size {block_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create datasets, VPDA and model\n",
    "\n",
    "from origami.model import ORIGAMI\n",
    "from origami.model.vpda import ObjectVPDA\n",
    "from origami.preprocessing import DFDataset\n",
    "from origami.utils import ModelConfig, TrainConfig\n",
    "\n",
    "# model and train configs\n",
    "model_config = ModelConfig.from_preset(\"medium\")\n",
    "model_config.position_encoding = \"NONE\"\n",
    "model_config.vocab_size = encoder.vocab_size\n",
    "model_config.block_size = block_size\n",
    "\n",
    "train_config = TrainConfig()\n",
    "train_config.learning_rate = 1e-3\n",
    "train_config.n_warmup_batches = 1000\n",
    "train_config.print_every = 100\n",
    "train_config.eval_every = 1000\n",
    "\n",
    "# datasets\n",
    "train_dataset = DFDataset(train_df)\n",
    "test_dataset = DFDataset(test_df)\n",
    "\n",
    "vpda = ObjectVPDA(encoder, schema)\n",
    "model = ORIGAMI(model_config, train_config, vpda=vpda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|  step: 0  |  epoch: 0  |  batch_num: 0  |  batch_dt: 0.00  |  batch_loss: 2.8099  |  lr: 1.01e-06  |  train_acc: 0.0000  |  test_loss: 2.7963  |  test_acc: 0.0000  |\n",
      "|  step: 1  |  epoch: 1  |  batch_num: 100  |  batch_dt: 69.12  |  batch_loss: 1.2363  |  lr: 1.01e-04  |\n",
      "|  step: 2  |  epoch: 2  |  batch_num: 200  |  batch_dt: 72.31  |  batch_loss: 0.7872  |  lr: 2.01e-04  |\n",
      "|  step: 3  |  epoch: 3  |  batch_num: 300  |  batch_dt: 75.93  |  batch_loss: 0.6798  |  lr: 3.01e-04  |\n",
      "|  step: 4  |  epoch: 5  |  batch_num: 400  |  batch_dt: 77.41  |  batch_loss: 0.6413  |  lr: 4.01e-04  |\n",
      "|  step: 5  |  epoch: 6  |  batch_num: 500  |  batch_dt: 77.41  |  batch_loss: 0.6531  |  lr: 5.01e-04  |\n",
      "|  step: 6  |  epoch: 7  |  batch_num: 600  |  batch_dt: 80.36  |  batch_loss: 0.6199  |  lr: 6.01e-04  |\n",
      "|  step: 7  |  epoch: 8  |  batch_num: 700  |  batch_dt: 80.23  |  batch_loss: 0.6177  |  lr: 7.01e-04  |\n",
      "|  step: 8  |  epoch: 10  |  batch_num: 800  |  batch_dt: 80.93  |  batch_loss: 0.6192  |  lr: 8.01e-04  |\n",
      "|  step: 9  |  epoch: 11  |  batch_num: 900  |  batch_dt: 83.41  |  batch_loss: 0.6183  |  lr: 9.01e-04  |\n",
      "|  step: 10  |  epoch: 12  |  batch_num: 1000  |  batch_dt: 79.50  |  batch_loss: 0.6174  |  lr: 1.00e-03  |  train_acc: 0.2800  |  test_loss: 0.6149  |  test_acc: 0.2700  |\n",
      "|  step: 11  |  epoch: 13  |  batch_num: 1100  |  batch_dt: 85.07  |  batch_loss: 0.6167  |  lr: 9.83e-04  |\n",
      "|  step: 12  |  epoch: 15  |  batch_num: 1200  |  batch_dt: 81.88  |  batch_loss: 0.6162  |  lr: 9.67e-04  |\n",
      "|  step: 13  |  epoch: 16  |  batch_num: 1300  |  batch_dt: 85.06  |  batch_loss: 0.6184  |  lr: 9.50e-04  |\n",
      "|  step: 14  |  epoch: 17  |  batch_num: 1400  |  batch_dt: 84.14  |  batch_loss: 0.6165  |  lr: 9.34e-04  |\n",
      "|  step: 15  |  epoch: 18  |  batch_num: 1500  |  batch_dt: 84.55  |  batch_loss: 0.6162  |  lr: 9.17e-04  |\n",
      "|  step: 16  |  epoch: 20  |  batch_num: 1600  |  batch_dt: 89.24  |  batch_loss: 0.6163  |  lr: 9.01e-04  |\n",
      "|  step: 17  |  epoch: 21  |  batch_num: 1700  |  batch_dt: 83.93  |  batch_loss: 0.6147  |  lr: 8.84e-04  |\n",
      "|  step: 18  |  epoch: 22  |  batch_num: 1800  |  batch_dt: 81.86  |  batch_loss: 0.6154  |  lr: 8.68e-04  |\n",
      "|  step: 19  |  epoch: 23  |  batch_num: 1900  |  batch_dt: 84.06  |  batch_loss: 0.6158  |  lr: 8.51e-04  |\n",
      "|  step: 20  |  epoch: 25  |  batch_num: 2000  |  batch_dt: 84.18  |  batch_loss: 0.6157  |  lr: 8.35e-04  |  train_acc: 0.2400  |  test_loss: 0.6157  |  test_acc: 0.3200  |\n",
      "|  step: 21  |  epoch: 26  |  batch_num: 2100  |  batch_dt: 82.62  |  batch_loss: 0.6145  |  lr: 8.18e-04  |\n",
      "|  step: 22  |  epoch: 27  |  batch_num: 2200  |  batch_dt: 82.26  |  batch_loss: 0.6144  |  lr: 8.02e-04  |\n",
      "|  step: 23  |  epoch: 28  |  batch_num: 2300  |  batch_dt: 84.51  |  batch_loss: 0.6146  |  lr: 7.85e-04  |\n",
      "|  step: 24  |  epoch: 30  |  batch_num: 2400  |  batch_dt: 88.43  |  batch_loss: 0.6122  |  lr: 7.69e-04  |\n",
      "|  step: 25  |  epoch: 31  |  batch_num: 2500  |  batch_dt: 84.77  |  batch_loss: 0.6064  |  lr: 7.52e-04  |\n",
      "|  step: 26  |  epoch: 32  |  batch_num: 2600  |  batch_dt: 81.80  |  batch_loss: 0.6076  |  lr: 7.36e-04  |\n",
      "|  step: 27  |  epoch: 33  |  batch_num: 2700  |  batch_dt: 83.84  |  batch_loss: 0.6037  |  lr: 7.19e-04  |\n",
      "|  step: 28  |  epoch: 35  |  batch_num: 2800  |  batch_dt: 86.09  |  batch_loss: 0.5997  |  lr: 7.03e-04  |\n",
      "|  step: 29  |  epoch: 36  |  batch_num: 2900  |  batch_dt: 84.66  |  batch_loss: 0.6003  |  lr: 6.86e-04  |\n",
      "|  step: 30  |  epoch: 37  |  batch_num: 3000  |  batch_dt: 83.94  |  batch_loss: 0.5997  |  lr: 6.70e-04  |  train_acc: 0.9900  |  test_loss: 0.6012  |  test_acc: 0.9700  |\n",
      "|  step: 31  |  epoch: 38  |  batch_num: 3100  |  batch_dt: 85.54  |  batch_loss: 0.6016  |  lr: 6.53e-04  |\n",
      "|  step: 32  |  epoch: 40  |  batch_num: 3200  |  batch_dt: 87.08  |  batch_loss: 0.6020  |  lr: 6.37e-04  |\n",
      "|  step: 33  |  epoch: 41  |  batch_num: 3300  |  batch_dt: 84.96  |  batch_loss: 0.6008  |  lr: 6.20e-04  |\n",
      "|  step: 34  |  epoch: 42  |  batch_num: 3400  |  batch_dt: 81.66  |  batch_loss: 0.6001  |  lr: 6.04e-04  |\n",
      "|  step: 35  |  epoch: 43  |  batch_num: 3500  |  batch_dt: 85.94  |  batch_loss: 0.5990  |  lr: 5.87e-04  |\n",
      "|  step: 36  |  epoch: 45  |  batch_num: 3600  |  batch_dt: 86.59  |  batch_loss: 0.5995  |  lr: 5.71e-04  |\n",
      "|  step: 37  |  epoch: 46  |  batch_num: 3700  |  batch_dt: 86.63  |  batch_loss: 0.5981  |  lr: 5.54e-04  |\n",
      "|  step: 38  |  epoch: 47  |  batch_num: 3800  |  batch_dt: 80.56  |  batch_loss: 0.5990  |  lr: 5.38e-04  |\n",
      "|  step: 39  |  epoch: 48  |  batch_num: 3900  |  batch_dt: 85.43  |  batch_loss: 0.5989  |  lr: 5.21e-04  |\n",
      "|  step: 40  |  epoch: 50  |  batch_num: 4000  |  batch_dt: 86.06  |  batch_loss: 0.6012  |  lr: 5.05e-04  |  train_acc: 1.0000  |  test_loss: 0.5994  |  test_acc: 0.9600  |\n",
      "|  step: 41  |  epoch: 51  |  batch_num: 4100  |  batch_dt: 84.70  |  batch_loss: 0.5984  |  lr: 4.88e-04  |\n",
      "|  step: 42  |  epoch: 52  |  batch_num: 4200  |  batch_dt: 85.25  |  batch_loss: 0.5984  |  lr: 4.72e-04  |\n",
      "|  step: 43  |  epoch: 53  |  batch_num: 4300  |  batch_dt: 86.57  |  batch_loss: 0.5987  |  lr: 4.55e-04  |\n",
      "|  step: 44  |  epoch: 55  |  batch_num: 4400  |  batch_dt: 87.32  |  batch_loss: 0.5995  |  lr: 4.39e-04  |\n",
      "|  step: 45  |  epoch: 56  |  batch_num: 4500  |  batch_dt: 83.58  |  batch_loss: 0.5984  |  lr: 4.22e-04  |\n",
      "|  step: 46  |  epoch: 57  |  batch_num: 4600  |  batch_dt: 85.26  |  batch_loss: 0.5972  |  lr: 4.06e-04  |\n",
      "|  step: 47  |  epoch: 58  |  batch_num: 4700  |  batch_dt: 85.96  |  batch_loss: 0.5990  |  lr: 3.89e-04  |\n",
      "|  step: 48  |  epoch: 60  |  batch_num: 4800  |  batch_dt: 87.25  |  batch_loss: 0.5994  |  lr: 3.73e-04  |\n",
      "|  step: 49  |  epoch: 61  |  batch_num: 4900  |  batch_dt: 83.92  |  batch_loss: 0.5973  |  lr: 3.56e-04  |\n"
     ]
    }
   ],
   "source": [
    "from origami.inference import Predictor\n",
    "from origami.utils import make_progress_callback\n",
    "\n",
    "# create a predictor\n",
    "predictor = Predictor(model, encoder, TARGET_FIELD)\n",
    "\n",
    "# create and register progress callback\n",
    "progress_callback = make_progress_callback(\n",
    "    train_config, train_dataset=train_dataset, test_dataset=test_dataset, predictor=predictor\n",
    ")\n",
    "model.set_callback(\"on_batch_end\", progress_callback)\n",
    "\n",
    "# train model (train and test accuracy should start to go towards 1.0 after ~3000 batches as loss drops below 0.6)\n",
    "model.train_model(train_dataset, batches=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70b85aaa81f546bdb6884a00476d383f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting:   0%|          | 0/85 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.9815\n",
      "Model predictions:  ['spellbooks', 'gold', 'gold', 'spellbooks', 'spellbooks', 'gold', 'artifacts', 'spellbooks', 'gold', 'gemstones']\n",
      "Correct labels:  ['spellbooks', 'gold', 'gold', 'spellbooks', 'spellbooks', 'gold', 'artifacts', 'spellbooks', 'gold', 'gemstones']\n"
     ]
    }
   ],
   "source": [
    "# calculate test accuracy\n",
    "acc = predictor.accuracy(test_dataset, show_progress=True)\n",
    "print(f\"Test accuracy: {acc:.4f}\")\n",
    "\n",
    "# we can also access the predictions with the `predict()` method\n",
    "predictions = predictor.predict(test_dataset)\n",
    "print(\"Model predictions: \", predictions[:10])\n",
    "print(\"Correct labels: \", test_dataset.df[\"target\"].to_list()[:10])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
