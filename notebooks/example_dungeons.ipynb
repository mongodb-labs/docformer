{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training an ORiGAMi model on the Dungeons dataset\n",
    "\n",
    "The Dungeons dataset is a (dungeons-themed) challenging synthetic dataset for supervised classification on\n",
    "semi-structured data.\n",
    "\n",
    "Each instance constains a corridor array with several rooms. Each room has a door number and contains multiple\n",
    "treasure chests with different-colored keys. All but one of the treasures are fake though.\n",
    "\n",
    "The goal is to find the correct room number and key color in each dungeon based on some clues and return the\n",
    "only non-fake treasure.\n",
    "\n",
    "The clues are given at the top-level of the object with keys `door`, `key_color`.\n",
    "\n",
    "To make it even harder, the `corridor` array may be shuffled, and room objects may have a number of monsters as\n",
    "their first field, shifting the token positions of the serialized object by a variable amount.\n",
    "\n",
    "The following dictionary represents one example JSON instance:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"door\": 1, // clue which door is the correct one\n",
    "  \"key_color\": \"blue\", // clue which key is the correct one\n",
    "  \"corridor\": [\n",
    "    {\n",
    "      \"monsters\": [\"troll\", \"wolf\"], // optional monsters in front of the door\n",
    "      \"door_no\": 1, // door number in the corridor\n",
    "      \"red_key\": \"gemstones\", // different keys return different treasures,\n",
    "      \"blue_key\": \"spellbooks\", // but only one is real, the others are fake\n",
    "      \"green_key\": \"artifacts\"\n",
    "    },\n",
    "    {\n",
    "      // another room\n",
    "      \"door_no\": 0, // rooms can be shuffled, here room 0 comes after 1\n",
    "      \"red_key\": \"diamonds\",\n",
    "      \"blue_key\": \"gold\",\n",
    "      \"green_key\": \"gemstones\"\n",
    "    }\n",
    "    // ... more doors ...\n",
    "  ],\n",
    "  \"treasure\": \"spellbooks\" // correct treasure (target label)\n",
    "}\n",
    "```\n",
    "\n",
    "The correct answer for this instance is \"spellbooks\", because the `door` is 1 and the `key_color` is \"blue\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"door\": 1,\n",
      "  \"key_color\": \"green\",\n",
      "  \"corridor\": [\n",
      "    {\n",
      "      \"door_no\": 0,\n",
      "      \"red_key\": \"gemstones\",\n",
      "      \"blue_key\": \"spellbooks\",\n",
      "      \"green_key\": \"diamonds\"\n",
      "    },\n",
      "    {\n",
      "      \"monsters\": [\n",
      "        \"dragon\",\n",
      "        \"orc\"\n",
      "      ],\n",
      "      \"door_no\": 1,\n",
      "      \"red_key\": \"gemstones\",\n",
      "      \"blue_key\": \"diamonds\",\n",
      "      \"green_key\": \"gold\"\n",
      "    },\n",
      "    {\n",
      "      \"monsters\": [\n",
      "        \"troll\"\n",
      "      ],\n",
      "      \"door_no\": 2,\n",
      "      \"red_key\": \"artifacts\",\n",
      "      \"blue_key\": \"diamonds\",\n",
      "      \"green_key\": \"spellbooks\"\n",
      "    },\n",
      "    {\n",
      "      \"monsters\": [\n",
      "        \"goblin\"\n",
      "      ],\n",
      "      \"door_no\": 3,\n",
      "      \"red_key\": \"artifacts\",\n",
      "      \"blue_key\": \"gemstones\",\n",
      "      \"green_key\": \"spellbooks\"\n",
      "    },\n",
      "    {\n",
      "      \"door_no\": 4,\n",
      "      \"red_key\": \"gemstones\",\n",
      "      \"blue_key\": \"gemstones\",\n",
      "      \"green_key\": \"spellbooks\"\n",
      "    }\n",
      "  ],\n",
      "  \"treasure\": \"gold\"\n",
      "}\n",
      "len train: 8000, len test: 2000\n",
      "vocab size 53\n",
      "block size 148\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from origami.datasets.dungeons import generate_data\n",
    "from origami.preprocessing import (\n",
    "    DocTokenizerPipe,\n",
    "    PadTruncTokensPipe,\n",
    "    SchemaParserPipe,\n",
    "    TargetFieldPipe,\n",
    "    TokenEncoderPipe,\n",
    "    docs_to_df,\n",
    ")\n",
    "\n",
    "# generate Dungeons dataset (see origami/datasets/dungeons.py)\n",
    "data = generate_data(\n",
    "    num_instances=10_000,\n",
    "    num_doors_range=(5, 10),\n",
    "    num_colors=3,\n",
    "    num_treasures=5,\n",
    "    with_monsters=True,  # makes it harder as token positions get shifted by variable amount\n",
    "    shuffle_rooms=True,  # makes it harder because rooms are in random order\n",
    ")\n",
    "\n",
    "# print example dictionary\n",
    "print(json.dumps(data[0], indent=2))\n",
    "\n",
    "# load data into dataframe and split into train/test\n",
    "df = docs_to_df(data)\n",
    "train_docs_df, test_docs_df = train_test_split(df, test_size=0.2, shuffle=True)\n",
    "\n",
    "TARGET_FIELD = \"treasure\"\n",
    "\n",
    "# create train and test pipelines\n",
    "pipes = {\n",
    "    \"schema\": SchemaParserPipe(),\n",
    "    \"target\": TargetFieldPipe(TARGET_FIELD),\n",
    "    \"tokenizer\": DocTokenizerPipe(path_in_field_tokens=True),\n",
    "    \"padding\": PadTruncTokensPipe(length=\"max\"),\n",
    "    \"encoder\": TokenEncoderPipe(),\n",
    "}\n",
    "\n",
    "pipeline = Pipeline([(name, pipes[name]) for name in (\"schema\", \"target\", \"tokenizer\", \"padding\", \"encoder\")])\n",
    "\n",
    "# process train, eval and test data\n",
    "train_df = pipeline.fit_transform(train_docs_df)\n",
    "test_df = pipeline.transform(test_docs_df)\n",
    "\n",
    "# get stateful objects\n",
    "schema = pipes[\"schema\"].schema\n",
    "encoder = pipes[\"encoder\"].encoder\n",
    "block_size = pipes[\"padding\"].length\n",
    "\n",
    "# print data stats\n",
    "print(f\"len train: {len(train_df)}, len test: {len(test_df)}\")\n",
    "print(f\"vocab size {encoder.vocab_size}\")\n",
    "print(f\"block size {block_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create datasets, VPDA and model\n",
    "\n",
    "from origami.model import ORIGAMI\n",
    "from origami.model.vpda import ObjectVPDA\n",
    "from origami.preprocessing import DFDataset\n",
    "from origami.utils import ModelConfig, TrainConfig\n",
    "\n",
    "# model and train configs\n",
    "model_config = ModelConfig.from_preset(\"medium\")  # see origami/utils/config.py for different presets\n",
    "model_config.vocab_size = encoder.vocab_size\n",
    "model_config.block_size = block_size\n",
    "\n",
    "train_config = TrainConfig()\n",
    "train_config.learning_rate = 1e-3\n",
    "\n",
    "# datasets\n",
    "train_dataset = DFDataset(train_df)\n",
    "test_dataset = DFDataset(test_df)\n",
    "\n",
    "vpda = ObjectVPDA(encoder, schema)\n",
    "model = ORIGAMI(model_config, train_config, vpda=vpda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|  step: 0  |  epoch: 0  |  batch_num: 0  |  batch_dt: 0.00  |  batch_loss: 2.5856  |  lr: 1.01e-06  |  train_acc: 0.2100  |  test_loss: 2.5782  |  test_acc: 0.1600  |\n",
      "|  step: 1  |  epoch: 1  |  batch_num: 100  |  batch_dt: 122.21  |  batch_loss: 1.1130  |  lr: 1.01e-04  |\n",
      "|  step: 2  |  epoch: 2  |  batch_num: 200  |  batch_dt: 130.67  |  batch_loss: 0.8074  |  lr: 2.01e-04  |\n",
      "|  step: 3  |  epoch: 3  |  batch_num: 300  |  batch_dt: 127.39  |  batch_loss: 0.7669  |  lr: 3.01e-04  |\n",
      "|  step: 4  |  epoch: 5  |  batch_num: 400  |  batch_dt: 115.76  |  batch_loss: 0.7419  |  lr: 4.01e-04  |\n",
      "|  step: 5  |  epoch: 6  |  batch_num: 500  |  batch_dt: 119.28  |  batch_loss: 0.7220  |  lr: 5.01e-04  |\n",
      "|  step: 6  |  epoch: 7  |  batch_num: 600  |  batch_dt: 116.48  |  batch_loss: 0.7229  |  lr: 6.01e-04  |\n",
      "|  step: 7  |  epoch: 8  |  batch_num: 700  |  batch_dt: 118.20  |  batch_loss: 0.7161  |  lr: 7.01e-04  |\n",
      "|  step: 8  |  epoch: 10  |  batch_num: 800  |  batch_dt: 117.42  |  batch_loss: 0.7183  |  lr: 8.01e-04  |\n",
      "|  step: 9  |  epoch: 11  |  batch_num: 900  |  batch_dt: 116.75  |  batch_loss: 0.7162  |  lr: 9.01e-04  |\n",
      "|  step: 10  |  epoch: 12  |  batch_num: 1000  |  batch_dt: 118.17  |  batch_loss: 0.7147  |  lr: 1.00e-03  |  train_acc: 0.2800  |  test_loss: 0.7119  |  test_acc: 0.1700  |\n",
      "|  step: 11  |  epoch: 13  |  batch_num: 1100  |  batch_dt: 115.67  |  batch_loss: 0.7180  |  lr: 9.95e-04  |\n",
      "|  step: 12  |  epoch: 15  |  batch_num: 1200  |  batch_dt: 121.48  |  batch_loss: 0.7159  |  lr: 9.91e-04  |\n",
      "|  step: 13  |  epoch: 16  |  batch_num: 1300  |  batch_dt: 118.52  |  batch_loss: 0.7131  |  lr: 9.86e-04  |\n",
      "|  step: 14  |  epoch: 17  |  batch_num: 1400  |  batch_dt: 132.98  |  batch_loss: 0.7156  |  lr: 9.81e-04  |\n",
      "|  step: 15  |  epoch: 18  |  batch_num: 1500  |  batch_dt: 117.92  |  batch_loss: 0.7155  |  lr: 9.76e-04  |\n",
      "|  step: 16  |  epoch: 20  |  batch_num: 1600  |  batch_dt: 116.03  |  batch_loss: 0.7124  |  lr: 9.72e-04  |\n",
      "|  step: 17  |  epoch: 21  |  batch_num: 1700  |  batch_dt: 116.84  |  batch_loss: 0.7157  |  lr: 9.67e-04  |\n",
      "|  step: 18  |  epoch: 22  |  batch_num: 1800  |  batch_dt: 114.62  |  batch_loss: 0.7123  |  lr: 9.62e-04  |\n",
      "|  step: 19  |  epoch: 23  |  batch_num: 1900  |  batch_dt: 116.46  |  batch_loss: 0.7105  |  lr: 9.58e-04  |\n",
      "|  step: 20  |  epoch: 25  |  batch_num: 2000  |  batch_dt: 116.47  |  batch_loss: 0.7128  |  lr: 9.53e-04  |  train_acc: 0.3600  |  test_loss: 0.7103  |  test_acc: 0.3100  |\n",
      "|  step: 21  |  epoch: 26  |  batch_num: 2100  |  batch_dt: 116.46  |  batch_loss: 0.7123  |  lr: 9.48e-04  |\n",
      "|  step: 22  |  epoch: 27  |  batch_num: 2200  |  batch_dt: 116.19  |  batch_loss: 0.7073  |  lr: 9.43e-04  |\n",
      "|  step: 23  |  epoch: 28  |  batch_num: 2300  |  batch_dt: 115.77  |  batch_loss: 0.7081  |  lr: 9.39e-04  |\n",
      "|  step: 24  |  epoch: 30  |  batch_num: 2400  |  batch_dt: 117.09  |  batch_loss: 0.7106  |  lr: 9.34e-04  |\n",
      "|  step: 25  |  epoch: 31  |  batch_num: 2500  |  batch_dt: 119.48  |  batch_loss: 0.7042  |  lr: 9.29e-04  |\n",
      "|  step: 26  |  epoch: 32  |  batch_num: 2600  |  batch_dt: 108.58  |  batch_loss: 0.7055  |  lr: 9.25e-04  |\n",
      "|  step: 27  |  epoch: 33  |  batch_num: 2700  |  batch_dt: 120.45  |  batch_loss: 0.7065  |  lr: 9.20e-04  |\n",
      "|  step: 28  |  epoch: 35  |  batch_num: 2800  |  batch_dt: 117.79  |  batch_loss: 0.7004  |  lr: 9.15e-04  |\n",
      "|  step: 29  |  epoch: 36  |  batch_num: 2900  |  batch_dt: 116.13  |  batch_loss: 0.7062  |  lr: 9.10e-04  |\n",
      "|  step: 30  |  epoch: 37  |  batch_num: 3000  |  batch_dt: 124.71  |  batch_loss: 0.6997  |  lr: 9.06e-04  |  train_acc: 0.7400  |  test_loss: 0.7118  |  test_acc: 0.7200  |\n",
      "|  step: 31  |  epoch: 38  |  batch_num: 3100  |  batch_dt: 114.71  |  batch_loss: 0.6960  |  lr: 9.01e-04  |\n",
      "|  step: 32  |  epoch: 40  |  batch_num: 3200  |  batch_dt: 116.09  |  batch_loss: 0.6937  |  lr: 8.96e-04  |\n",
      "|  step: 33  |  epoch: 41  |  batch_num: 3300  |  batch_dt: 165.06  |  batch_loss: 0.6924  |  lr: 8.92e-04  |\n",
      "|  step: 34  |  epoch: 42  |  batch_num: 3400  |  batch_dt: 138.75  |  batch_loss: 0.6888  |  lr: 8.87e-04  |\n",
      "|  step: 35  |  epoch: 43  |  batch_num: 3500  |  batch_dt: 147.76  |  batch_loss: 0.6894  |  lr: 8.82e-04  |\n",
      "|  step: 36  |  epoch: 45  |  batch_num: 3600  |  batch_dt: 141.68  |  batch_loss: 0.6814  |  lr: 8.77e-04  |\n",
      "|  step: 37  |  epoch: 46  |  batch_num: 3700  |  batch_dt: 117.07  |  batch_loss: 0.6845  |  lr: 8.73e-04  |\n",
      "|  step: 38  |  epoch: 47  |  batch_num: 3800  |  batch_dt: 139.54  |  batch_loss: 0.6843  |  lr: 8.68e-04  |\n",
      "|  step: 39  |  epoch: 48  |  batch_num: 3900  |  batch_dt: 152.54  |  batch_loss: 0.6859  |  lr: 8.63e-04  |\n",
      "|  step: 40  |  epoch: 50  |  batch_num: 4000  |  batch_dt: 139.87  |  batch_loss: 0.6761  |  lr: 8.59e-04  |  train_acc: 1.0000  |  test_loss: 0.7153  |  test_acc: 1.0000  |\n",
      "|  step: 41  |  epoch: 51  |  batch_num: 4100  |  batch_dt: 158.02  |  batch_loss: 0.6758  |  lr: 8.54e-04  |\n",
      "|  step: 42  |  epoch: 52  |  batch_num: 4200  |  batch_dt: 116.98  |  batch_loss: 0.6803  |  lr: 8.49e-04  |\n",
      "|  step: 43  |  epoch: 53  |  batch_num: 4300  |  batch_dt: 119.48  |  batch_loss: 0.6785  |  lr: 8.44e-04  |\n",
      "|  step: 44  |  epoch: 55  |  batch_num: 4400  |  batch_dt: 128.71  |  batch_loss: 0.6767  |  lr: 8.40e-04  |\n",
      "|  step: 45  |  epoch: 56  |  batch_num: 4500  |  batch_dt: 148.31  |  batch_loss: 0.6738  |  lr: 8.35e-04  |\n",
      "|  step: 46  |  epoch: 57  |  batch_num: 4600  |  batch_dt: 146.81  |  batch_loss: 0.6737  |  lr: 8.30e-04  |\n",
      "|  step: 47  |  epoch: 58  |  batch_num: 4700  |  batch_dt: 159.02  |  batch_loss: 0.6765  |  lr: 8.26e-04  |\n",
      "|  step: 48  |  epoch: 60  |  batch_num: 4800  |  batch_dt: 137.63  |  batch_loss: 0.6673  |  lr: 8.21e-04  |\n",
      "|  step: 49  |  epoch: 61  |  batch_num: 4900  |  batch_dt: 160.75  |  batch_loss: 0.6659  |  lr: 8.16e-04  |\n",
      "|  step: 50  |  epoch: 62  |  batch_num: 5000  |  batch_dt: 168.90  |  batch_loss: 0.6745  |  lr: 8.11e-04  |  train_acc: 1.0000  |  test_loss: 0.7269  |  test_acc: 1.0000  |\n",
      "|  step: 51  |  epoch: 63  |  batch_num: 5100  |  batch_dt: 161.75  |  batch_loss: 0.6666  |  lr: 8.07e-04  |\n",
      "|  step: 52  |  epoch: 65  |  batch_num: 5200  |  batch_dt: 123.61  |  batch_loss: 0.6625  |  lr: 8.02e-04  |\n",
      "|  step: 53  |  epoch: 66  |  batch_num: 5300  |  batch_dt: 129.68  |  batch_loss: 0.6585  |  lr: 7.97e-04  |\n",
      "|  step: 54  |  epoch: 67  |  batch_num: 5400  |  batch_dt: 166.59  |  batch_loss: 0.6637  |  lr: 7.93e-04  |\n",
      "|  step: 55  |  epoch: 68  |  batch_num: 5500  |  batch_dt: 163.24  |  batch_loss: 0.6671  |  lr: 7.88e-04  |\n",
      "|  step: 56  |  epoch: 70  |  batch_num: 5600  |  batch_dt: 147.88  |  batch_loss: 0.6580  |  lr: 7.83e-04  |\n",
      "|  step: 57  |  epoch: 71  |  batch_num: 5700  |  batch_dt: 160.68  |  batch_loss: 0.6538  |  lr: 7.78e-04  |\n",
      "|  step: 58  |  epoch: 72  |  batch_num: 5800  |  batch_dt: 161.05  |  batch_loss: 0.6614  |  lr: 7.74e-04  |\n",
      "|  step: 59  |  epoch: 73  |  batch_num: 5900  |  batch_dt: 161.55  |  batch_loss: 0.6564  |  lr: 7.69e-04  |\n",
      "|  step: 60  |  epoch: 75  |  batch_num: 6000  |  batch_dt: 160.02  |  batch_loss: 0.6472  |  lr: 7.64e-04  |  train_acc: 1.0000  |  test_loss: 0.7416  |  test_acc: 1.0000  |\n",
      "|  step: 61  |  epoch: 76  |  batch_num: 6100  |  batch_dt: 158.99  |  batch_loss: 0.6517  |  lr: 7.60e-04  |\n",
      "|  step: 62  |  epoch: 77  |  batch_num: 6200  |  batch_dt: 153.49  |  batch_loss: 0.6555  |  lr: 7.55e-04  |\n",
      "|  step: 63  |  epoch: 78  |  batch_num: 6300  |  batch_dt: 127.73  |  batch_loss: 0.6540  |  lr: 7.50e-04  |\n",
      "|  step: 64  |  epoch: 80  |  batch_num: 6400  |  batch_dt: 129.27  |  batch_loss: 0.6428  |  lr: 7.45e-04  |\n",
      "|  step: 65  |  epoch: 81  |  batch_num: 6500  |  batch_dt: 161.83  |  batch_loss: 0.6523  |  lr: 7.41e-04  |\n",
      "|  step: 66  |  epoch: 82  |  batch_num: 6600  |  batch_dt: 157.60  |  batch_loss: 0.6450  |  lr: 7.36e-04  |\n",
      "|  step: 67  |  epoch: 83  |  batch_num: 6700  |  batch_dt: 160.35  |  batch_loss: 0.6529  |  lr: 7.31e-04  |\n",
      "|  step: 68  |  epoch: 85  |  batch_num: 6800  |  batch_dt: 167.61  |  batch_loss: 0.6380  |  lr: 7.27e-04  |\n",
      "|  step: 69  |  epoch: 86  |  batch_num: 6900  |  batch_dt: 139.07  |  batch_loss: 0.6483  |  lr: 7.22e-04  |\n",
      "|  step: 70  |  epoch: 87  |  batch_num: 7000  |  batch_dt: 157.79  |  batch_loss: 0.6415  |  lr: 7.17e-04  |  train_acc: 1.0000  |  test_loss: 0.7609  |  test_acc: 1.0000  |\n",
      "|  step: 71  |  epoch: 88  |  batch_num: 7100  |  batch_dt: 147.11  |  batch_loss: 0.6417  |  lr: 7.12e-04  |\n",
      "|  step: 72  |  epoch: 90  |  batch_num: 7200  |  batch_dt: 126.82  |  batch_loss: 0.6413  |  lr: 7.08e-04  |\n",
      "|  step: 73  |  epoch: 91  |  batch_num: 7300  |  batch_dt: 159.85  |  batch_loss: 0.6387  |  lr: 7.03e-04  |\n",
      "|  step: 74  |  epoch: 92  |  batch_num: 7400  |  batch_dt: 122.38  |  batch_loss: 0.6355  |  lr: 6.98e-04  |\n",
      "|  step: 75  |  epoch: 93  |  batch_num: 7500  |  batch_dt: 156.48  |  batch_loss: 0.6416  |  lr: 6.94e-04  |\n",
      "|  step: 76  |  epoch: 95  |  batch_num: 7600  |  batch_dt: 156.37  |  batch_loss: 0.6322  |  lr: 6.89e-04  |\n",
      "|  step: 77  |  epoch: 96  |  batch_num: 7700  |  batch_dt: 150.30  |  batch_loss: 0.6321  |  lr: 6.84e-04  |\n",
      "|  step: 78  |  epoch: 97  |  batch_num: 7800  |  batch_dt: 167.79  |  batch_loss: 0.6353  |  lr: 6.79e-04  |\n",
      "|  step: 79  |  epoch: 98  |  batch_num: 7900  |  batch_dt: 160.08  |  batch_loss: 0.6330  |  lr: 6.75e-04  |\n",
      "|  step: 80  |  epoch: 100  |  batch_num: 8000  |  batch_dt: 124.51  |  batch_loss: 0.6270  |  lr: 6.70e-04  |  train_acc: 1.0000  |  test_loss: 0.7729  |  test_acc: 1.0000  |\n",
      "|  step: 81  |  epoch: 101  |  batch_num: 8100  |  batch_dt: 122.15  |  batch_loss: 0.6292  |  lr: 6.65e-04  |\n",
      "|  step: 82  |  epoch: 102  |  batch_num: 8200  |  batch_dt: 124.34  |  batch_loss: 0.6278  |  lr: 6.61e-04  |\n",
      "|  step: 83  |  epoch: 103  |  batch_num: 8300  |  batch_dt: 124.70  |  batch_loss: 0.6306  |  lr: 6.56e-04  |\n",
      "|  step: 84  |  epoch: 105  |  batch_num: 8400  |  batch_dt: 120.64  |  batch_loss: 0.6245  |  lr: 6.51e-04  |\n",
      "|  step: 85  |  epoch: 106  |  batch_num: 8500  |  batch_dt: 165.44  |  batch_loss: 0.6221  |  lr: 6.46e-04  |\n",
      "|  step: 86  |  epoch: 107  |  batch_num: 8600  |  batch_dt: 180.93  |  batch_loss: 0.6298  |  lr: 6.42e-04  |\n",
      "|  step: 87  |  epoch: 108  |  batch_num: 8700  |  batch_dt: 181.70  |  batch_loss: 0.6226  |  lr: 6.37e-04  |\n",
      "|  step: 88  |  epoch: 110  |  batch_num: 8800  |  batch_dt: 145.59  |  batch_loss: 0.6187  |  lr: 6.32e-04  |\n",
      "|  step: 89  |  epoch: 111  |  batch_num: 8900  |  batch_dt: 153.60  |  batch_loss: 0.6228  |  lr: 6.28e-04  |\n",
      "|  step: 90  |  epoch: 112  |  batch_num: 9000  |  batch_dt: 178.77  |  batch_loss: 0.6267  |  lr: 6.23e-04  |  train_acc: 1.0000  |  test_loss: 0.7681  |  test_acc: 1.0000  |\n",
      "|  step: 91  |  epoch: 113  |  batch_num: 9100  |  batch_dt: 156.96  |  batch_loss: 0.6168  |  lr: 6.18e-04  |\n",
      "|  step: 92  |  epoch: 115  |  batch_num: 9200  |  batch_dt: 151.27  |  batch_loss: 0.6146  |  lr: 6.13e-04  |\n",
      "|  step: 93  |  epoch: 116  |  batch_num: 9300  |  batch_dt: 171.27  |  batch_loss: 0.6152  |  lr: 6.09e-04  |\n",
      "|  step: 94  |  epoch: 117  |  batch_num: 9400  |  batch_dt: 153.52  |  batch_loss: 0.6261  |  lr: 6.04e-04  |\n",
      "|  step: 95  |  epoch: 118  |  batch_num: 9500  |  batch_dt: 157.80  |  batch_loss: 0.6221  |  lr: 5.99e-04  |\n",
      "|  step: 96  |  epoch: 120  |  batch_num: 9600  |  batch_dt: 157.18  |  batch_loss: 0.6094  |  lr: 5.95e-04  |\n",
      "|  step: 97  |  epoch: 121  |  batch_num: 9700  |  batch_dt: 154.20  |  batch_loss: 0.6173  |  lr: 5.90e-04  |\n",
      "|  step: 98  |  epoch: 122  |  batch_num: 9800  |  batch_dt: 126.84  |  batch_loss: 0.6119  |  lr: 5.85e-04  |\n",
      "|  step: 99  |  epoch: 123  |  batch_num: 9900  |  batch_dt: 164.57  |  batch_loss: 0.6236  |  lr: 5.80e-04  |\n",
      "|  step: 100  |  epoch: 125  |  batch_num: 10000  |  batch_dt: 152.82  |  batch_loss: 0.6071  |  lr: 5.76e-04  |  train_acc: 1.0000  |  test_loss: 0.7926  |  test_acc: 1.0000  |\n",
      "|  step: 101  |  epoch: 126  |  batch_num: 10100  |  batch_dt: 160.93  |  batch_loss: 0.6095  |  lr: 5.71e-04  |\n",
      "|  step: 102  |  epoch: 127  |  batch_num: 10200  |  batch_dt: 166.99  |  batch_loss: 0.6149  |  lr: 5.66e-04  |\n",
      "|  step: 103  |  epoch: 128  |  batch_num: 10300  |  batch_dt: 160.49  |  batch_loss: 0.6118  |  lr: 5.62e-04  |\n",
      "|  step: 104  |  epoch: 130  |  batch_num: 10400  |  batch_dt: 138.96  |  batch_loss: 0.6013  |  lr: 5.57e-04  |\n",
      "|  step: 105  |  epoch: 131  |  batch_num: 10500  |  batch_dt: 117.89  |  batch_loss: 0.6124  |  lr: 5.52e-04  |\n",
      "|  step: 106  |  epoch: 132  |  batch_num: 10600  |  batch_dt: 173.68  |  batch_loss: 0.6100  |  lr: 5.47e-04  |\n",
      "|  step: 107  |  epoch: 133  |  batch_num: 10700  |  batch_dt: 143.08  |  batch_loss: 0.6108  |  lr: 5.43e-04  |\n",
      "|  step: 108  |  epoch: 135  |  batch_num: 10800  |  batch_dt: 155.21  |  batch_loss: 0.6016  |  lr: 5.38e-04  |\n",
      "|  step: 109  |  epoch: 136  |  batch_num: 10900  |  batch_dt: 186.51  |  batch_loss: 0.6010  |  lr: 5.33e-04  |\n",
      "|  step: 110  |  epoch: 137  |  batch_num: 11000  |  batch_dt: 187.30  |  batch_loss: 0.6052  |  lr: 5.29e-04  |  train_acc: 1.0000  |  test_loss: 0.7903  |  test_acc: 1.0000  |\n",
      "|  step: 111  |  epoch: 138  |  batch_num: 11100  |  batch_dt: 175.39  |  batch_loss: 0.6160  |  lr: 5.24e-04  |\n",
      "|  step: 112  |  epoch: 140  |  batch_num: 11200  |  batch_dt: 123.40  |  batch_loss: 0.5992  |  lr: 5.19e-04  |\n",
      "|  step: 113  |  epoch: 141  |  batch_num: 11300  |  batch_dt: 156.83  |  batch_loss: 0.6027  |  lr: 5.14e-04  |\n",
      "|  step: 114  |  epoch: 142  |  batch_num: 11400  |  batch_dt: 136.81  |  batch_loss: 0.6058  |  lr: 5.10e-04  |\n",
      "|  step: 115  |  epoch: 143  |  batch_num: 11500  |  batch_dt: 155.97  |  batch_loss: 0.6041  |  lr: 5.05e-04  |\n",
      "|  step: 116  |  epoch: 145  |  batch_num: 11600  |  batch_dt: 180.75  |  batch_loss: 0.5983  |  lr: 5.00e-04  |\n",
      "|  step: 117  |  epoch: 146  |  batch_num: 11700  |  batch_dt: 172.53  |  batch_loss: 0.5963  |  lr: 4.96e-04  |\n",
      "|  step: 118  |  epoch: 147  |  batch_num: 11800  |  batch_dt: 155.11  |  batch_loss: 0.6070  |  lr: 4.91e-04  |\n",
      "|  step: 119  |  epoch: 148  |  batch_num: 11900  |  batch_dt: 154.42  |  batch_loss: 0.5981  |  lr: 4.86e-04  |\n",
      "|  step: 120  |  epoch: 150  |  batch_num: 12000  |  batch_dt: 136.75  |  batch_loss: 0.5890  |  lr: 4.81e-04  |  train_acc: 1.0000  |  test_loss: 0.8053  |  test_acc: 1.0000  |\n",
      "|  step: 121  |  epoch: 151  |  batch_num: 12100  |  batch_dt: 169.80  |  batch_loss: 0.5959  |  lr: 4.77e-04  |\n",
      "|  step: 122  |  epoch: 152  |  batch_num: 12200  |  batch_dt: 131.41  |  batch_loss: 0.6004  |  lr: 4.72e-04  |\n",
      "|  step: 123  |  epoch: 153  |  batch_num: 12300  |  batch_dt: 154.69  |  batch_loss: 0.6018  |  lr: 4.67e-04  |\n",
      "|  step: 124  |  epoch: 155  |  batch_num: 12400  |  batch_dt: 134.05  |  batch_loss: 0.5997  |  lr: 4.63e-04  |\n",
      "|  step: 125  |  epoch: 156  |  batch_num: 12500  |  batch_dt: 152.87  |  batch_loss: 0.5919  |  lr: 4.58e-04  |\n",
      "|  step: 126  |  epoch: 157  |  batch_num: 12600  |  batch_dt: 133.32  |  batch_loss: 0.5898  |  lr: 4.53e-04  |\n",
      "|  step: 127  |  epoch: 158  |  batch_num: 12700  |  batch_dt: 164.58  |  batch_loss: 0.5989  |  lr: 4.48e-04  |\n",
      "|  step: 128  |  epoch: 160  |  batch_num: 12800  |  batch_dt: 156.73  |  batch_loss: 0.5927  |  lr: 4.44e-04  |\n",
      "|  step: 129  |  epoch: 161  |  batch_num: 12900  |  batch_dt: 174.72  |  batch_loss: 0.5960  |  lr: 4.39e-04  |\n",
      "|  step: 130  |  epoch: 162  |  batch_num: 13000  |  batch_dt: 139.95  |  batch_loss: 0.5898  |  lr: 4.34e-04  |  train_acc: 1.0000  |  test_loss: 0.8066  |  test_acc: 1.0000  |\n",
      "|  step: 131  |  epoch: 163  |  batch_num: 13100  |  batch_dt: 130.73  |  batch_loss: 0.5962  |  lr: 4.30e-04  |\n",
      "|  step: 132  |  epoch: 165  |  batch_num: 13200  |  batch_dt: 176.96  |  batch_loss: 0.5799  |  lr: 4.25e-04  |\n",
      "|  step: 133  |  epoch: 166  |  batch_num: 13300  |  batch_dt: 159.10  |  batch_loss: 0.5956  |  lr: 4.20e-04  |\n",
      "|  step: 134  |  epoch: 167  |  batch_num: 13400  |  batch_dt: 167.89  |  batch_loss: 0.5942  |  lr: 4.15e-04  |\n",
      "|  step: 135  |  epoch: 168  |  batch_num: 13500  |  batch_dt: 168.59  |  batch_loss: 0.5949  |  lr: 4.11e-04  |\n",
      "|  step: 136  |  epoch: 170  |  batch_num: 13600  |  batch_dt: 159.67  |  batch_loss: 0.5867  |  lr: 4.06e-04  |\n",
      "|  step: 137  |  epoch: 171  |  batch_num: 13700  |  batch_dt: 148.20  |  batch_loss: 0.5887  |  lr: 4.01e-04  |\n",
      "|  step: 138  |  epoch: 172  |  batch_num: 13800  |  batch_dt: 199.60  |  batch_loss: 0.5880  |  lr: 3.97e-04  |\n",
      "|  step: 139  |  epoch: 173  |  batch_num: 13900  |  batch_dt: 165.29  |  batch_loss: 0.5846  |  lr: 3.92e-04  |\n",
      "|  step: 140  |  epoch: 175  |  batch_num: 14000  |  batch_dt: 196.82  |  batch_loss: 0.5846  |  lr: 3.87e-04  |  train_acc: 1.0000  |  test_loss: 0.8412  |  test_acc: 1.0000  |\n",
      "|  step: 141  |  epoch: 176  |  batch_num: 14100  |  batch_dt: 166.23  |  batch_loss: 0.5799  |  lr: 3.82e-04  |\n",
      "|  step: 142  |  epoch: 177  |  batch_num: 14200  |  batch_dt: 183.57  |  batch_loss: 0.5858  |  lr: 3.78e-04  |\n",
      "|  step: 143  |  epoch: 178  |  batch_num: 14300  |  batch_dt: 170.41  |  batch_loss: 0.5875  |  lr: 3.73e-04  |\n",
      "|  step: 144  |  epoch: 180  |  batch_num: 14400  |  batch_dt: 155.98  |  batch_loss: 0.5896  |  lr: 3.68e-04  |\n",
      "|  step: 145  |  epoch: 181  |  batch_num: 14500  |  batch_dt: 129.64  |  batch_loss: 0.5821  |  lr: 3.64e-04  |\n",
      "|  step: 146  |  epoch: 182  |  batch_num: 14600  |  batch_dt: 145.60  |  batch_loss: 0.5840  |  lr: 3.59e-04  |\n",
      "|  step: 147  |  epoch: 183  |  batch_num: 14700  |  batch_dt: 181.03  |  batch_loss: 0.5867  |  lr: 3.54e-04  |\n",
      "|  step: 148  |  epoch: 185  |  batch_num: 14800  |  batch_dt: 168.85  |  batch_loss: 0.5736  |  lr: 3.49e-04  |\n",
      "|  step: 149  |  epoch: 186  |  batch_num: 14900  |  batch_dt: 163.18  |  batch_loss: 0.5782  |  lr: 3.45e-04  |\n",
      "|  step: 150  |  epoch: 187  |  batch_num: 15000  |  batch_dt: 175.61  |  batch_loss: 0.5701  |  lr: 3.40e-04  |  train_acc: 1.0000  |  test_loss: 0.8407  |  test_acc: 1.0000  |\n",
      "|  step: 151  |  epoch: 188  |  batch_num: 15100  |  batch_dt: 179.98  |  batch_loss: 0.5795  |  lr: 3.35e-04  |\n",
      "|  step: 152  |  epoch: 190  |  batch_num: 15200  |  batch_dt: 117.21  |  batch_loss: 0.5717  |  lr: 3.31e-04  |\n",
      "|  step: 153  |  epoch: 191  |  batch_num: 15300  |  batch_dt: 141.47  |  batch_loss: 0.5843  |  lr: 3.26e-04  |\n",
      "|  step: 154  |  epoch: 192  |  batch_num: 15400  |  batch_dt: 186.96  |  batch_loss: 0.5848  |  lr: 3.21e-04  |\n",
      "|  step: 155  |  epoch: 193  |  batch_num: 15500  |  batch_dt: 162.25  |  batch_loss: 0.5868  |  lr: 3.16e-04  |\n",
      "|  step: 156  |  epoch: 195  |  batch_num: 15600  |  batch_dt: 163.23  |  batch_loss: 0.5693  |  lr: 3.12e-04  |\n",
      "|  step: 157  |  epoch: 196  |  batch_num: 15700  |  batch_dt: 150.59  |  batch_loss: 0.5753  |  lr: 3.07e-04  |\n",
      "|  step: 158  |  epoch: 197  |  batch_num: 15800  |  batch_dt: 165.88  |  batch_loss: 0.5713  |  lr: 3.02e-04  |\n",
      "|  step: 159  |  epoch: 198  |  batch_num: 15900  |  batch_dt: 149.95  |  batch_loss: 0.5846  |  lr: 2.98e-04  |\n",
      "|  step: 160  |  epoch: 200  |  batch_num: 16000  |  batch_dt: 182.61  |  batch_loss: 0.5754  |  lr: 2.93e-04  |  train_acc: 1.0000  |  test_loss: 0.8364  |  test_acc: 1.0000  |\n",
      "|  step: 161  |  epoch: 201  |  batch_num: 16100  |  batch_dt: 155.12  |  batch_loss: 0.5794  |  lr: 2.88e-04  |\n",
      "|  step: 162  |  epoch: 202  |  batch_num: 16200  |  batch_dt: 178.79  |  batch_loss: 0.5714  |  lr: 2.83e-04  |\n",
      "|  step: 163  |  epoch: 203  |  batch_num: 16300  |  batch_dt: 176.07  |  batch_loss: 0.5762  |  lr: 2.79e-04  |\n",
      "|  step: 164  |  epoch: 205  |  batch_num: 16400  |  batch_dt: 204.71  |  batch_loss: 0.5687  |  lr: 2.74e-04  |\n",
      "|  step: 165  |  epoch: 206  |  batch_num: 16500  |  batch_dt: 155.92  |  batch_loss: 0.5703  |  lr: 2.69e-04  |\n",
      "|  step: 166  |  epoch: 207  |  batch_num: 16600  |  batch_dt: 127.72  |  batch_loss: 0.5758  |  lr: 2.65e-04  |\n",
      "|  step: 167  |  epoch: 208  |  batch_num: 16700  |  batch_dt: 165.61  |  batch_loss: 0.5731  |  lr: 2.60e-04  |\n",
      "|  step: 168  |  epoch: 210  |  batch_num: 16800  |  batch_dt: 149.88  |  batch_loss: 0.5643  |  lr: 2.55e-04  |\n",
      "|  step: 169  |  epoch: 211  |  batch_num: 16900  |  batch_dt: 180.54  |  batch_loss: 0.5628  |  lr: 2.50e-04  |\n",
      "|  step: 170  |  epoch: 212  |  batch_num: 17000  |  batch_dt: 156.89  |  batch_loss: 0.5747  |  lr: 2.46e-04  |  train_acc: 1.0000  |  test_loss: 0.8504  |  test_acc: 1.0000  |\n",
      "|  step: 171  |  epoch: 213  |  batch_num: 17100  |  batch_dt: 139.65  |  batch_loss: 0.5732  |  lr: 2.41e-04  |\n",
      "|  step: 172  |  epoch: 215  |  batch_num: 17200  |  batch_dt: 157.32  |  batch_loss: 0.5628  |  lr: 2.36e-04  |\n",
      "|  step: 173  |  epoch: 216  |  batch_num: 17300  |  batch_dt: 123.39  |  batch_loss: 0.5722  |  lr: 2.32e-04  |\n",
      "|  step: 174  |  epoch: 217  |  batch_num: 17400  |  batch_dt: 158.55  |  batch_loss: 0.5747  |  lr: 2.27e-04  |\n",
      "|  step: 175  |  epoch: 218  |  batch_num: 17500  |  batch_dt: 153.08  |  batch_loss: 0.5708  |  lr: 2.22e-04  |\n",
      "|  step: 176  |  epoch: 220  |  batch_num: 17600  |  batch_dt: 152.13  |  batch_loss: 0.5633  |  lr: 2.17e-04  |\n",
      "|  step: 177  |  epoch: 221  |  batch_num: 17700  |  batch_dt: 162.49  |  batch_loss: 0.5660  |  lr: 2.13e-04  |\n",
      "|  step: 178  |  epoch: 222  |  batch_num: 17800  |  batch_dt: 189.99  |  batch_loss: 0.5617  |  lr: 2.08e-04  |\n",
      "|  step: 179  |  epoch: 223  |  batch_num: 17900  |  batch_dt: 162.17  |  batch_loss: 0.5705  |  lr: 2.03e-04  |\n",
      "|  step: 180  |  epoch: 225  |  batch_num: 18000  |  batch_dt: 165.53  |  batch_loss: 0.5649  |  lr: 1.99e-04  |  train_acc: 1.0000  |  test_loss: 0.8388  |  test_acc: 1.0000  |\n",
      "|  step: 181  |  epoch: 226  |  batch_num: 18100  |  batch_dt: 182.16  |  batch_loss: 0.5686  |  lr: 1.94e-04  |\n",
      "|  step: 182  |  epoch: 227  |  batch_num: 18200  |  batch_dt: 177.00  |  batch_loss: 0.5713  |  lr: 1.89e-04  |\n",
      "|  step: 183  |  epoch: 228  |  batch_num: 18300  |  batch_dt: 148.61  |  batch_loss: 0.5670  |  lr: 1.84e-04  |\n",
      "|  step: 184  |  epoch: 230  |  batch_num: 18400  |  batch_dt: 186.23  |  batch_loss: 0.5706  |  lr: 1.80e-04  |\n",
      "|  step: 185  |  epoch: 231  |  batch_num: 18500  |  batch_dt: 156.15  |  batch_loss: 0.5595  |  lr: 1.75e-04  |\n",
      "|  step: 186  |  epoch: 232  |  batch_num: 18600  |  batch_dt: 175.47  |  batch_loss: 0.5700  |  lr: 1.70e-04  |\n",
      "|  step: 187  |  epoch: 233  |  batch_num: 18700  |  batch_dt: 133.33  |  batch_loss: 0.5647  |  lr: 1.66e-04  |\n",
      "|  step: 188  |  epoch: 235  |  batch_num: 18800  |  batch_dt: 163.00  |  batch_loss: 0.5622  |  lr: 1.61e-04  |\n",
      "|  step: 189  |  epoch: 236  |  batch_num: 18900  |  batch_dt: 176.68  |  batch_loss: 0.5583  |  lr: 1.56e-04  |\n",
      "|  step: 190  |  epoch: 237  |  batch_num: 19000  |  batch_dt: 168.16  |  batch_loss: 0.5640  |  lr: 1.51e-04  |  train_acc: 1.0000  |  test_loss: 0.8531  |  test_acc: 1.0000  |\n",
      "|  step: 191  |  epoch: 238  |  batch_num: 19100  |  batch_dt: 186.16  |  batch_loss: 0.5643  |  lr: 1.47e-04  |\n",
      "|  step: 192  |  epoch: 240  |  batch_num: 19200  |  batch_dt: 254.44  |  batch_loss: 0.5646  |  lr: 1.42e-04  |\n",
      "|  step: 193  |  epoch: 241  |  batch_num: 19300  |  batch_dt: 230.71  |  batch_loss: 0.5583  |  lr: 1.37e-04  |\n",
      "|  step: 194  |  epoch: 242  |  batch_num: 19400  |  batch_dt: 221.06  |  batch_loss: 0.5578  |  lr: 1.33e-04  |\n",
      "|  step: 195  |  epoch: 243  |  batch_num: 19500  |  batch_dt: 267.52  |  batch_loss: 0.5543  |  lr: 1.28e-04  |\n",
      "|  step: 196  |  epoch: 245  |  batch_num: 19600  |  batch_dt: 176.73  |  batch_loss: 0.5613  |  lr: 1.23e-04  |\n",
      "|  step: 197  |  epoch: 246  |  batch_num: 19700  |  batch_dt: 158.19  |  batch_loss: 0.5600  |  lr: 1.18e-04  |\n",
      "|  step: 198  |  epoch: 247  |  batch_num: 19800  |  batch_dt: 169.35  |  batch_loss: 0.5568  |  lr: 1.14e-04  |\n",
      "|  step: 199  |  epoch: 248  |  batch_num: 19900  |  batch_dt: 143.58  |  batch_loss: 0.5663  |  lr: 1.09e-04  |\n"
     ]
    }
   ],
   "source": [
    "from origami.inference import Predictor\n",
    "from origami.utils import make_progress_callback\n",
    "\n",
    "# create a predictor\n",
    "predictor = Predictor(model, encoder, TARGET_FIELD)\n",
    "\n",
    "# create and register progress callback\n",
    "progress_callback = make_progress_callback(\n",
    "    train_config, train_dataset=train_dataset, test_dataset=test_dataset, predictor=predictor\n",
    ")\n",
    "model.set_callback(\"on_batch_end\", progress_callback)\n",
    "\n",
    "# train model (train and test accuracy should start to go towards 1.0 after ~3000 batches as loss drops below 0.6)\n",
    "model.train_model(train_dataset, batches=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate test accuracy\n",
    "acc = predictor.accuracy(test_dataset, show_progress=True)\n",
    "print(f\"Test accuracy: {acc:.4f}\")\n",
    "\n",
    "# we can also access the predictions with the `predict()` method\n",
    "predictions = predictor.predict(test_dataset)\n",
    "print(\"Model predictions: \", predictions[:10])\n",
    "print(\"Correct labels: \", test_dataset.df[\"target\"].to_list()[:10])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
