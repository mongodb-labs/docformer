{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train STORM model on Dungeons dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"door\": 1,\n",
      "  \"key_color\": \"blue\",\n",
      "  \"corridor\": [\n",
      "    {\n",
      "      \"door_no\": 0,\n",
      "      \"red_key\": \"artifacts\",\n",
      "      \"blue_key\": \"artifacts\",\n",
      "      \"green_key\": \"gemstones\"\n",
      "    },\n",
      "    {\n",
      "      \"monsters\": [\n",
      "        \"goblin\",\n",
      "        \"orc\"\n",
      "      ],\n",
      "      \"door_no\": 1,\n",
      "      \"red_key\": \"artifacts\",\n",
      "      \"blue_key\": \"spellbooks\",\n",
      "      \"green_key\": \"diamonds\"\n",
      "    },\n",
      "    {\n",
      "      \"door_no\": 2,\n",
      "      \"red_key\": \"gemstones\",\n",
      "      \"blue_key\": \"diamonds\",\n",
      "      \"green_key\": \"artifacts\"\n",
      "    },\n",
      "    {\n",
      "      \"monsters\": [\n",
      "        \"troll\"\n",
      "      ],\n",
      "      \"door_no\": 3,\n",
      "      \"red_key\": \"artifacts\",\n",
      "      \"blue_key\": \"artifacts\",\n",
      "      \"green_key\": \"gold\"\n",
      "    },\n",
      "    {\n",
      "      \"monsters\": [\n",
      "        \"goblin\",\n",
      "        \"dragon\"\n",
      "      ],\n",
      "      \"door_no\": 4,\n",
      "      \"red_key\": \"artifacts\",\n",
      "      \"blue_key\": \"gold\",\n",
      "      \"green_key\": \"gemstones\"\n",
      "    },\n",
      "    {\n",
      "      \"door_no\": 5,\n",
      "      \"red_key\": \"diamonds\",\n",
      "      \"blue_key\": \"artifacts\",\n",
      "      \"green_key\": \"artifacts\"\n",
      "    },\n",
      "    {\n",
      "      \"door_no\": 6,\n",
      "      \"red_key\": \"artifacts\",\n",
      "      \"blue_key\": \"artifacts\",\n",
      "      \"green_key\": \"gemstones\"\n",
      "    },\n",
      "    {\n",
      "      \"monsters\": [\n",
      "        \"goblin\",\n",
      "        \"dragon\"\n",
      "      ],\n",
      "      \"door_no\": 7,\n",
      "      \"red_key\": \"gemstones\",\n",
      "      \"blue_key\": \"gemstones\",\n",
      "      \"green_key\": \"gemstones\"\n",
      "    }\n",
      "  ],\n",
      "  \"treasure\": \"spellbooks\"\n",
      "}\n",
      "len train: 8000, len test: 2000\n",
      "vocab size 53\n",
      "block size 149\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from storm_ml.datasets.dungeons import generate_data\n",
    "from storm_ml.preprocessing import (\n",
    "    DocTokenizerPipe,\n",
    "    PadTruncTokensPipe,\n",
    "    SchemaParserPipe,\n",
    "    TargetFieldPipe,\n",
    "    TokenEncoderPipe,\n",
    "    docs_to_df,\n",
    ")\n",
    "\n",
    "# generate Dungeons dataset (see storm_ml/datasets/dungeons.py)\n",
    "data = generate_data(\n",
    "    num_instances=10_000,\n",
    "    num_doors_range=(5, 10),\n",
    "    num_colors=3,\n",
    "    with_monsters=True,\n",
    "    num_treasures=5,\n",
    ")\n",
    "\n",
    "# print example dictionary\n",
    "print(json.dumps(data[0], indent=2))\n",
    "\n",
    "# load data into dataframe and split into train/test\n",
    "df = docs_to_df(data)\n",
    "train_docs_df, test_docs_df = train_test_split(df, test_size=0.2, shuffle=True)\n",
    "\n",
    "TARGET_FIELD = \"treasure\"\n",
    "\n",
    "# create train and test pipelines\n",
    "pipes = {\n",
    "    \"schema\": SchemaParserPipe(),\n",
    "    \"target\": TargetFieldPipe(TARGET_FIELD),\n",
    "    \"tokenizer\": DocTokenizerPipe(path_in_field_tokens=True),\n",
    "    \"padding\": PadTruncTokensPipe(length=\"max\"),\n",
    "    \"encoder\": TokenEncoderPipe(),\n",
    "}\n",
    "\n",
    "pipeline = Pipeline([(name, pipes[name]) for name in (\"schema\", \"target\", \"tokenizer\", \"padding\", \"encoder\")])\n",
    "\n",
    "# process train, eval and test data\n",
    "train_df = pipeline.fit_transform(train_docs_df)\n",
    "test_df = pipeline.transform(test_docs_df)\n",
    "\n",
    "# get stateful objects\n",
    "schema = pipes[\"schema\"].schema\n",
    "encoder = pipes[\"encoder\"].encoder\n",
    "block_size = pipes[\"padding\"].length\n",
    "\n",
    "# print data stats\n",
    "print(f\"len train: {len(train_df)}, len test: {len(test_df)}\")\n",
    "print(f\"vocab size {encoder.vocab_size}\")\n",
    "print(f\"block size {block_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running on device mps\n",
      "number of parameters: 0.81M\n"
     ]
    }
   ],
   "source": [
    "# create datasets, VPDA and model\n",
    "\n",
    "from storm_ml.model import STORM\n",
    "from storm_ml.model.vpda import DocumentVPDA\n",
    "from storm_ml.preprocessing import DFDataset\n",
    "from storm_ml.utils import ModelConfig, TrainConfig\n",
    "\n",
    "# model and train configs\n",
    "model_config = ModelConfig.from_preset(\"gpt-micro\")\n",
    "model_config.position_encoding = \"DOCUMENT\"\n",
    "model_config.vocab_size = encoder.vocab_size\n",
    "model_config.block_size = block_size\n",
    "\n",
    "train_config = TrainConfig()\n",
    "train_config.learning_rate = 1e-3\n",
    "train_config.n_warmup_batches = 1000\n",
    "\n",
    "# datasets\n",
    "train_dataset = DFDataset(train_df)\n",
    "test_dataset = DFDataset(test_df)\n",
    "\n",
    "vpda = DocumentVPDA(encoder, schema)\n",
    "model = STORM(model_config, train_config, vpda=vpda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|  step: 0  |  epoch: 0  |  batch_num: 0  |  batch_dt: 0.00  |  batch_loss: 2.7857  |  test_loss: 2.7821  |  test_acc: 0.0000  |  lr: 1.01e-06  |\n",
      "|  step: 1  |  epoch: 1  |  batch_num: 100  |  batch_dt: 118.92  |  batch_loss: 1.1267  |  test_loss: 1.0890  |  test_acc: 0.1400  |  lr: 1.01e-04  |\n",
      "|  step: 2  |  epoch: 2  |  batch_num: 200  |  batch_dt: 113.07  |  batch_loss: 0.7149  |  test_loss: 0.6951  |  test_acc: 0.1700  |  lr: 2.01e-04  |\n",
      "|  step: 3  |  epoch: 3  |  batch_num: 300  |  batch_dt: 113.26  |  batch_loss: 0.6304  |  test_loss: 0.6246  |  test_acc: 0.1900  |  lr: 3.01e-04  |\n",
      "|  step: 4  |  epoch: 5  |  batch_num: 400  |  batch_dt: 117.31  |  batch_loss: 0.6216  |  test_loss: 0.6188  |  test_acc: 0.1600  |  lr: 4.01e-04  |\n",
      "|  step: 5  |  epoch: 6  |  batch_num: 500  |  batch_dt: 140.12  |  batch_loss: 0.6194  |  test_loss: 0.6197  |  test_acc: 0.2600  |  lr: 5.01e-04  |\n",
      "|  step: 6  |  epoch: 7  |  batch_num: 600  |  batch_dt: 141.17  |  batch_loss: 0.6185  |  test_loss: 0.6178  |  test_acc: 0.2200  |  lr: 6.01e-04  |\n",
      "|  step: 7  |  epoch: 8  |  batch_num: 700  |  batch_dt: 116.13  |  batch_loss: 0.6194  |  test_loss: 0.6186  |  test_acc: 0.2300  |  lr: 7.01e-04  |\n",
      "|  step: 8  |  epoch: 10  |  batch_num: 800  |  batch_dt: 114.13  |  batch_loss: 0.6187  |  test_loss: 0.6167  |  test_acc: 0.1900  |  lr: 8.01e-04  |\n",
      "|  step: 9  |  epoch: 11  |  batch_num: 900  |  batch_dt: 143.79  |  batch_loss: 0.6155  |  test_loss: 0.6141  |  test_acc: 0.2600  |  lr: 9.01e-04  |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/thomas/code/storm/.venv/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:232: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|  step: 10  |  epoch: 12  |  batch_num: 1000  |  batch_dt: 114.54  |  batch_loss: 0.6187  |  test_loss: 0.6198  |  test_acc: 0.2000  |  lr: 1.00e-03  |\n",
      "|  step: 11  |  epoch: 13  |  batch_num: 1100  |  batch_dt: 119.04  |  batch_loss: 0.6161  |  test_loss: 0.6171  |  test_acc: 0.2800  |  lr: 9.83e-04  |\n",
      "|  step: 12  |  epoch: 15  |  batch_num: 1200  |  batch_dt: 146.16  |  batch_loss: 0.6154  |  test_loss: 0.6166  |  test_acc: 0.2100  |  lr: 9.67e-04  |\n",
      "|  step: 13  |  epoch: 16  |  batch_num: 1300  |  batch_dt: 115.42  |  batch_loss: 0.6165  |  test_loss: 0.6156  |  test_acc: 0.1500  |  lr: 9.50e-04  |\n",
      "|  step: 14  |  epoch: 17  |  batch_num: 1400  |  batch_dt: 121.07  |  batch_loss: 0.6169  |  test_loss: 0.6176  |  test_acc: 0.1200  |  lr: 9.34e-04  |\n",
      "|  step: 15  |  epoch: 18  |  batch_num: 1500  |  batch_dt: 117.39  |  batch_loss: 0.6159  |  test_loss: 0.6171  |  test_acc: 0.1600  |  lr: 9.17e-04  |\n",
      "|  step: 16  |  epoch: 20  |  batch_num: 1600  |  batch_dt: 117.40  |  batch_loss: 0.6165  |  test_loss: 0.6179  |  test_acc: 0.2500  |  lr: 9.01e-04  |\n",
      "|  step: 17  |  epoch: 21  |  batch_num: 1700  |  batch_dt: 148.21  |  batch_loss: 0.6166  |  test_loss: 0.6177  |  test_acc: 0.2800  |  lr: 8.84e-04  |\n",
      "|  step: 18  |  epoch: 22  |  batch_num: 1800  |  batch_dt: 118.60  |  batch_loss: 0.6148  |  test_loss: 0.6175  |  test_acc: 0.2900  |  lr: 8.68e-04  |\n",
      "|  step: 19  |  epoch: 23  |  batch_num: 1900  |  batch_dt: 115.75  |  batch_loss: 0.6137  |  test_loss: 0.6158  |  test_acc: 0.3600  |  lr: 8.51e-04  |\n",
      "|  step: 20  |  epoch: 25  |  batch_num: 2000  |  batch_dt: 110.41  |  batch_loss: 0.6129  |  test_loss: 0.6137  |  test_acc: 0.4100  |  lr: 8.35e-04  |\n",
      "|  step: 21  |  epoch: 26  |  batch_num: 2100  |  batch_dt: 121.37  |  batch_loss: 0.6075  |  test_loss: 0.6063  |  test_acc: 0.9100  |  lr: 8.18e-04  |\n",
      "|  step: 22  |  epoch: 27  |  batch_num: 2200  |  batch_dt: 117.89  |  batch_loss: 0.6022  |  test_loss: 0.6007  |  test_acc: 1.0000  |  lr: 8.02e-04  |\n",
      "|  step: 23  |  epoch: 28  |  batch_num: 2300  |  batch_dt: 116.62  |  batch_loss: 0.6015  |  test_loss: 0.6001  |  test_acc: 1.0000  |  lr: 7.85e-04  |\n",
      "|  step: 24  |  epoch: 30  |  batch_num: 2400  |  batch_dt: 120.63  |  batch_loss: 0.6008  |  test_loss: 0.6027  |  test_acc: 1.0000  |  lr: 7.69e-04  |\n",
      "|  step: 25  |  epoch: 31  |  batch_num: 2500  |  batch_dt: 118.29  |  batch_loss: 0.6020  |  test_loss: 0.6008  |  test_acc: 1.0000  |  lr: 7.52e-04  |\n",
      "|  step: 26  |  epoch: 32  |  batch_num: 2600  |  batch_dt: 114.43  |  batch_loss: 0.5984  |  test_loss: 0.6034  |  test_acc: 1.0000  |  lr: 7.36e-04  |\n",
      "|  step: 27  |  epoch: 33  |  batch_num: 2700  |  batch_dt: 116.68  |  batch_loss: 0.6003  |  test_loss: 0.6026  |  test_acc: 1.0000  |  lr: 7.19e-04  |\n",
      "|  step: 28  |  epoch: 35  |  batch_num: 2800  |  batch_dt: 121.44  |  batch_loss: 0.5982  |  test_loss: 0.6036  |  test_acc: 1.0000  |  lr: 7.03e-04  |\n",
      "model training interrupted after 35 epochs (2805 batches total).\n"
     ]
    }
   ],
   "source": [
    "from storm_ml.inference import Predictor\n",
    "from storm_ml.utils.guild import print_guild_scalars\n",
    "\n",
    "# create a predictor\n",
    "predictor = Predictor(model, encoder, TARGET_FIELD)\n",
    "\n",
    "\n",
    "# model callback during training, prints training and test metrics\n",
    "def progress_callback(model):\n",
    "    if model.batch_num % train_config.eval_every == 0:\n",
    "        print_guild_scalars(\n",
    "            step=f\"{int(model.batch_num / train_config.eval_every)}\",\n",
    "            epoch=model.epoch_num,\n",
    "            batch_num=model.batch_num,\n",
    "            batch_dt=f\"{model.batch_dt*1000:.2f}\",\n",
    "            batch_loss=f\"{model.loss:.4f}\",\n",
    "            test_loss=f\"{predictor.ce_loss(test_dataset.sample(n=100)):.4f}\",\n",
    "            test_acc=f\"{predictor.accuracy(test_dataset.sample(n=100)):.4f}\",\n",
    "            lr=f\"{model.learning_rate:.2e}\",\n",
    "        )\n",
    "\n",
    "\n",
    "model.set_callback(\"on_batch_end\", progress_callback)\n",
    "model.train_model(train_dataset, batches=5000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = predictor.accuracy(test_dataset)\n",
    "print(f\"Test accuracy: {acc:.4f}\")\n",
    "\n",
    "# we can also access the predictions with the `predict()` method\n",
    "predictions = predictor.predict(test_dataset)\n",
    "print(\"Model predictions: \", predictions[:10])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
